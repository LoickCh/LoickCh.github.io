<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Kaggle TPS.Nov.2022 | Loick Chambon</title> <meta name="author" content="Loick Chambon"/> <meta name="description" content="Tabular Playground Series - November 2022"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://loickch.github.io/blog/2023/challenge/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://loickch.github.io/"><span class="font-weight-bold">Loick</span> Chambon</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Kaggle TPS.Nov.2022</h1> <p class="post-meta">January 2, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/Kaggle"> <i class="fas fa-hashtag fa-sm"></i> Kaggle</a>     ·   <a href="/blog/category/challenge"> <i class="fas fa-tag fa-sm"></i> challenge</a>   </p> </header> <article class="post-content"> <h1 id="tps-nov-2022">TPS Nov-2022</h1> <p>Hello, in this note I explain my personnal solution to the TPS challenge where I ranked 18/689 (2.6 %).</p> <p> <br> </p> <h3 id="table-of-content">Table of content</h3> <ul> <li>Subject</li> <li>Approach</li> <li>Other solutions</li> <li>Conclusion</li> </ul> <p> <br> </p> <h2 id="1subject">1.Subject</h2> <p>In this competition, there is a collection of predictions and ground truth labels for a binary classification task we do not know about. The objective is to use blending techniques to improve the model’s predictions by combining submissions. Stacking and blending are machine learning techniques learning a meta-model:</p> <ul> <li>stacking: trains a meta-model from a pool of base models. The base models are trained on a complete training set, then the meta-model is trained on the features that are outputs of the base models. This operation can be repeated several times, at the risk of overfitting and having a more complex model.</li> <li>blending: trains a meta-model on a separate holdout set. It is very similar to stacking however, it does not use the same data.</li> </ul> <p>About the data format, submission files are divided in two parts. In the submissions folder, the first half of the rows contains training set predictions. Their name corresponds to the log-loss over the training set. The other half contains predictions for the testing set. The testing set consists of 20,000 rows, and the submission expects probabilities.</p> <p> <br> </p> <h2 id="2-approach">2. Approach</h2> <p>I am inspired by several notebooks and ideas from other participants. In this competition, exploratory data augmentation was not crucial so I did not focus on it. However, preparing data and chose a good training pipeline helps me to achieve my results.</p> <h3 id="21-pre-processing">2.1 Pre-processing</h3> <h4 id="211-data-format">2.1.1 Data-format</h4> <p>Instead of using a folder of submission files, we concatenate all of them in a single dataframe object containing 5k columns and 40k rows where each column is a submission and each row is an index. Then, to accelerate the loading, we convert the concatenated dataset to a binary format. In our case we select the feather (.ft) extension, but many exists: pickle (specific to Python), parquet, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_feather</span><span class="p">(</span><span class="sh">'</span><span class="s">../input/tps2022novfeather/train.ftr</span><span class="sh">'</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">../input/tps2022novfeather/train_labels.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Once loaded, we observed that some inputs are above 1 or below 0. Since it is supposed to contain probabilities, we clip values between 1e-7 (close to minimum float32 precision) and 1-1e-7 to ensure we manipulate probabilities. Alternatively, we could have flag set a value for outliers (for instance -1 below and 2 above) and use randomforest-like algorithms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Clip data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">EPS</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">EPS</span><span class="p">)</span>
</code></pre></div></div> <p>After that, we split our dataset in training and testing datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Copy and split datasets in two sets.
</span><span class="n">X</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">TRAIN_SIZE</span><span class="p">,:]),</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">TRAIN_SIZE</span><span class="p">:,:])</span>
<span class="n">y</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">TRAIN_SIZE</span><span class="p">]),</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">TRAIN_SIZE</span><span class="p">:])</span>
</code></pre></div></div> <p>Then, before to train models, we modify our datasets according to a bag of tricks coming from diverse sources.</p> <h4 id="212-bag-of-tricks">2.1.2 Bag of tricks</h4> <p>Since the submissions contain binary probabilities, we can consider whether it would be better to invert some of these probabilities in order to improve the log-loss score. We found that 9 elements had better log-loss scores when their probabilities were inverted, so we made this change.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">better_invert_proba_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><span class="nf">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">EPS</span><span class="p">)</span> <span class="o">-</span> <span class="nf">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">EPS</span><span class="p">))</span> 
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
        <span class="p">]</span>
        <span class="p">)</span><span class="o">&gt;</span> <span class="mi">0</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">better_invert_proba_idx</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">better_invert_proba_idx</span><span class="p">,:]</span>
</code></pre></div></div> <p>Then, we used logits instead of probabilities, even though it is generally not necessary or advisable to convert predicted probabilities into logits before solving a binary classification problem. This step is optional, but allows for better results when learning logistic regression according to <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/364013" target="_blank" rel="noopener noreferrer">this</a>.</p> <p>I only kept these two ideas in my final submission but many have been tested and discussed by the community. For instance, considering bad classifier are nearly random classifiers, they have a ROC-AUC scores close to 0.5. Other pre-processing strategies tested were to calibrate the probabilities using isotonic regression or an embedding of the inputs with a neural network.</p> <h3 id="22-model">2.2 Model</h3> <p>Like many other candidates, I used LightAutoML, a Python library performing automated machine learning with high GPU compatibility. There are several other automated machine learning libraries available, such as H2O, PyCaret, TPOT, and Autosklearn. Of these, only <a href="https://github.com/sb-ai-lab/LightAutoML" target="_blank" rel="noopener noreferrer">LightAutoML</a> and <a href="https://github.com/EpistasisLab/tpot" target="_blank" rel="noopener noreferrer">TPOT</a> produced good results for me. I didn’t spend much time on tuning the hyperparameters, so it’s possible that some of the other libraries may perform just as well or better if given more time and attention.</p> <h4 id="221-feature-selection">2.2.1 Feature selection</h4> <p>Feature selection is an important step in building a model. Reducing the number of features often reduces overfitting and speeds up training, especially with 5,000 columns. Ideally, the feature selection method should be independent of the model and should discriminate the data by itself. For instance, we can select features having a high variance or remove features highly correlated to ensure diversity and erase redundancy (for instance using hierarchical clustering on the Spearman rank-order correlations or the Jensen Shanon distance, <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/368905" target="_blank" rel="noopener noreferrer">cf</a>).</p> <p>Another approach consists on training a model to predict labels and then retrieve the most important features for predictions. It is even more effective when training algorithms with L1 or L2 regularisations, as they penalise the model useless model weights. Alternatively to feature selection, we could have applied dimensionality reduction techniques such as Principal Component Analysis or Partial Least Square Regression. It is another way to reduce the shape of our input data. In this competition, PCA, or neural network encoding do not lead to the best results, so I did not keep them.</p> <p>Since performing feature selection by training gave me the best results, I chose it even though it is more prone to overfitting. I tried two different libraries: scikit-learn (in combination with the catboost and LinearSVC algorithms) and LightAutoML set with few parameters. The final submission uses LightAutoML and more specifically the <code class="language-plaintext highlighter-rouge">TabularAutoML</code> class. My method is rather similar to the one developped by <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/363483" target="_blank" rel="noopener noreferrer">A.Ryzkhov</a>. First of all, we need to specify a task. The subject of the challenge is a blending problem, but seen from another angle, it is nothing more or less than a binary classification where each column is a submission. Then, we chose the selection_params:</p> <ul> <li>importance_type: we have the choice between permutation (calls <code class="language-plaintext highlighter-rouge">NpPermutationImportanceEstimator</code>) or gain (calls <code class="language-plaintext highlighter-rouge">ModelBasedImportanceEstimator</code>). Permutation uses a random permutation of elements in a single column for each feature to calculate its significance. Intuitively, if an element is significant, performance deteriorates when it is shuffled. A more advanced techniques compares the importance of features fitted to the target against the importance of features when fitted to noise defined as the shuffled target (<a href="https://www.kaggle.com/code/ogrellier/feature-selection-with-null-importances" target="_blank" rel="noopener noreferrer">cf.</a>).</li> <li>feature_group_size: specify the number of elements permuted during each step of the search. The less it is, the more accurate we are, but the longer it is.</li> <li>select_algos: <code class="language-plaintext highlighter-rouge">linear_l2</code> corresponds to LBFGS L2 regression based on torch while <code class="language-plaintext highlighter-rouge">gbm</code> corresponds to gradient boosting algorithms from LightGBM library. There are several others, but LightGBM is a good starting point because it allows for L1 and L2 regularisation, thus eliminating unnecessary features.</li> <li>mode: specify the feature selection mode between no selection (0), drop zero importances (1) and iterative permutation importances (2).</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">automl</span> <span class="o">=</span> <span class="nc">TabularAutoML</span><span class="p">(</span>
    <span class="n">task</span> <span class="o">=</span> <span class="nc">Task</span><span class="p">(</span><span class="sh">'</span><span class="s">binary</span><span class="sh">'</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="sh">'</span><span class="s">logloss</span><span class="sh">'</span><span class="p">),</span> 
    <span class="n">timeout</span> <span class="o">=</span> <span class="n">TIMEOUT</span><span class="p">,</span>
    <span class="n">cpu_limit</span> <span class="o">=</span> <span class="n">N_THREADS</span><span class="p">,</span>
    <span class="n">selection_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">importance_type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">permutation</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">fit_on_holdout</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">mode</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> 
        <span class="sh">'</span><span class="s">feature_group_size</span><span class="sh">'</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">select_algos</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">linear_l2</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">gbm</span><span class="sh">'</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="n">reader_params</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">n_jobs</span><span class="sh">'</span><span class="p">:</span> <span class="n">N_THREADS</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div> <p>The feature selection takes around 1h on Kaggle.</p> <h4 id="222-training">2.2.2 Training</h4> <p>I have trained and finetuned lightgbm classifiers, catboost classifiers and scikit-learn algorithms (HistGradientBoostingClassifier, LogisticRegression, etc.) using Optuna and even pure deep learning networks, but LightAutoML gave me the best results.</p> <p>The training loop relies again on the <code class="language-plaintext highlighter-rouge">TabularAutoML</code> class with few other hyperparameters:</p> <ul> <li>general_params: specifies which algorithms trained.</li> <li>nn_params: specifies the neural network parameters. The neural architecture is predifined and flagged with <code class="language-plaintext highlighter-rouge">mlp</code>, <code class="language-plaintext highlighter-rouge">dense</code>, <code class="language-plaintext highlighter-rouge">resnet</code> or <code class="language-plaintext highlighter-rouge">denselight</code> but we can custom it changing the activation function (or implementing new ones such as <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/366518" target="_blank" rel="noopener noreferrer">Growing Cosine Unit</a>, MISH), clipping gradients, select the batch size, etc.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># General neural network parameters
</span><span class="n">general_nn_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">random_state</span><span class="sh">"</span><span class="p">:</span><span class="n">SEED</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">act_fun</span><span class="sh">"</span><span class="p">:</span> <span class="n">Mish</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">n_epochs</span><span class="sh">"</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">bs</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">num_workers</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">path_to_save</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">clip_grad</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">clip_grad_params</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">max_norm</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> 
    <span class="sh">"</span><span class="s">verbose</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">pin_memory</span><span class="sh">"</span><span class="p">:</span><span class="bp">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Main class
</span><span class="n">automl</span> <span class="o">=</span> <span class="nc">TabularAutoML</span><span class="p">(</span>
    <span class="n">task</span> <span class="o">=</span> <span class="nc">Task</span><span class="p">(</span><span class="sh">'</span><span class="s">binary</span><span class="sh">'</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">logloss</span><span class="sh">'</span><span class="p">),</span> 
    <span class="n">cpu_limit</span> <span class="o">=</span> <span class="n">N_THREADS</span><span class="p">,</span>
    <span class="n">general_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">use_algos</span><span class="sh">"</span><span class="p">:</span> <span class="p">[[</span><span class="sh">"</span><span class="s">linear_l2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">mlp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">denselight</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">lgb</span><span class="sh">"</span><span class="p">]],</span> 
    <span class="p">},</span>
    <span class="n">nn_pipeline_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">use_qnt</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span>
    <span class="p">},</span>
    <span class="n">nn_params</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="o">**</span><span class="n">general_nn_params</span><span class="p">},</span>
        <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="o">**</span><span class="n">general_nn_params</span><span class="p">,</span> 
              <span class="o">**</span><span class="p">{</span>
                    <span class="sh">'</span><span class="s">bs</span><span class="sh">'</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
                    <span class="sh">'</span><span class="s">opt_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.04047</span><span class="p">,</span> <span class="sh">'</span><span class="s">weight_decay</span><span class="sh">'</span><span class="p">:</span> <span class="mf">2.43e-05</span><span class="p">},</span> 
                    <span class="sh">'</span><span class="s">clip_grad</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> 
                    <span class="sh">'</span><span class="s">clip_grad_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">max_norm</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0979877026847337</span><span class="p">}</span>
                <span class="p">}</span>
             <span class="p">},</span>
        <span class="sh">"</span><span class="s">2</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="o">**</span><span class="n">general_nn_params</span><span class="p">,</span> 
              <span class="o">**</span><span class="p">{</span>
                    <span class="sh">'</span><span class="s">bs</span><span class="sh">'</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                    <span class="sh">'</span><span class="s">opt_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.00543</span><span class="p">,</span><span class="sh">'</span><span class="s">weight_decay</span><span class="sh">'</span><span class="p">:</span> <span class="mf">2.2282e-05</span><span class="p">},</span>
                    <span class="sh">'</span><span class="s">clip_grad</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> 
                    <span class="sh">'</span><span class="s">clip_grad_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">max_norm</span><span class="sh">'</span><span class="p">:</span> <span class="mf">4.683639733744027</span><span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>
    <span class="p">},</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">max_tuning_time</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3600</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">fit_on_holdout</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span>
    <span class="p">},</span>
    <span class="n">reader_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">n_jobs</span><span class="sh">'</span><span class="p">:</span> <span class="n">N_THREADS</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Training
</span><span class="n">X</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span>
<span class="n">oof_pred</span> <span class="o">=</span> <span class="n">automl</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> 
    <span class="n">roles</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">},</span> 
    <span class="n">cv_iter</span><span class="o">=</span><span class="n">train_val_indices</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
</code></pre></div></div> <p>Once the model is trained, we simply predict the probabilities on the test set. If we have many submissions, a last step consists on averaging submissions. A good averaging strategy is to find the optimal weights for each submission on each k-fold sets. It can be done using <code class="language-plaintext highlighter-rouge">scipy.minimize</code> or even applied the whole pipeline previously developed, but the snake bites the tail at the risk of overfitting.</p> <h2 id="3-other-solutions">3. Other solutions</h2> <p>Some winner candidates have shared their solutions:</p> <h3 id="1st-place-solution"> <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/369674" target="_blank" rel="noopener noreferrer">1.st</a> place solution</h3> <p>Keypoints:</p> <ul> <li>Framework: LightAutoML</li> <li>Data: use logits instead of probabilities.</li> <li>Architecture: XGBoost, LightGBM, NNs (with MISH and Swish activation functions).</li> <li>Training: 10 k-Fold cross-validations. It seems to be good to increase the number of folds while doing cross-validation. However, if we increase it too much, we can overfit on our data.</li> <li>Ensembling: uses <a href="https://www.kaggle.com/code/pourchot/stacking-with-scipy-minimize" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">scipy.minimize</code></a> to find the optimal weights on the OOF.</li> </ul> <h3 id="3rd-place-solution">3.rd place solution</h3> <p>Keypoints:</p> <ul> <li>Framework: AutoGluon.</li> <li>Feature selection: drop features containing values out of range (0,1) and using Spearman correlation.</li> <li>Calibrations: uses Isotonic Calibration.</li> </ul> <h3 id="7th-place-solution"> <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/369731" target="_blank" rel="noopener noreferrer">7.th</a> place solution</h3> <p>Keypoints:</p> <ul> <li>Feature selection: drop features based on the difference between the loss value of the model with this feature and without it. Performs feature selection with <a href="https://www.kaggle.com/code/ogrellier/feature-selection-with-null-importances" target="_blank" rel="noopener noreferrer">null-importance</a>.</li> </ul> <h2 id="4-conclusion">4. Conclusion</h2> <p>This toy competition was original in a sense it consists on ensembling models to get better performances even if we tackle it as a binary classification task. In fact, we build a meta-model for a task we do not know about. Interestingly, auto machine learning frameworks work well and gave the best results. Many participants have used LightAutoML pushed by simple and effective notebooks written by the authors of the library. In this challenge, the exploratory data analysis was not so important, but having a good feature selection algorithm helped a lot.</p> <hr> <p>Other sources:</p> <ul> <li><a href="https://medium.com/@stevenyu530_73989/stacking-and-blending-intuitive-explanation-of-advanced-ensemble-methods-46b295da413c" target="_blank" rel="noopener noreferrer">Blending and stacking</a></li> <li><a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/364013" target="_blank" rel="noopener noreferrer">Calibration</a></li> </ul> <p>Interesting techniques not used:</p> <ul> <li><a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/364834" target="_blank" rel="noopener noreferrer">OOF Forward selection</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Loick Chambon. Created using the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>