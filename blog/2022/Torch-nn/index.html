<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Torch module explained (torch.nn) | Loick Chambon</title> <meta name="author" content="Loick Chambon"/> <meta name="description" content="Exploring and explaining torch functionalities."/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://loickch.github.io/blog/2022/Torch-nn/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://loickch.github.io/"><span class="font-weight-bold">Loick</span> Chambon</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Torch module explained (torch.nn)</h1> <p class="post-meta">October 23, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a>     ·   <a href="/blog/category/library"> <i class="fas fa-tag fa-sm"></i> library</a>   </p> </header> <article class="post-content"> <p>Today we will dive in torch.nn module, one of them most important Pytorch’s module to create neural networks and to customise existing code.</p> <p> <br> </p> <h1 id="table-of-contents">Table of contents:</h1> <p>Torch.nn introduces new concept and tries to answer the following questions:</p> <ul> <li>Notions</li> <li>How to create a model ?</li> <li>How to deploy a model ?</li> <li>How to inspect a model ?</li> </ul> <p> <br> </p> <h1 id="notions">Notions</h1> <h2 id="subclass-nnparameter">Subclass: nn.Parameter</h2> <p><strong>Sum up:</strong> nn.Parameter is a subclass of torch.Tensor used when we need to optimize tensors during the gradient descent. It automatically add tensors to the parameters() iterator allowing us to simply define an optimizer.</p> <p><strong>Example:</strong> To illustrate the notion, let us implement a linear layer. To recall, a linear layer modifies the number of channels of an input tensor <em>x</em> applying a single hidden layer. It is defined with a weight matrix <em>A</em> and bias matrix <em>b</em> by the following equation: \(y=x.A^T + b\) Where y has shape $(<em>,H_{out})$ and x has shape $(</em>,H_{in})$.</p> <p>Thus, to implement the layer, we need to define <em>A</em> and <em>b</em>, i.e to initialize a weight and a bias matrix and to specify that they need to be updated during optimization. A common practice to define an optimizer is to call <em>model.parameters()</em> returning an iterator with all model registered parameters:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SGD optimizer
# &gt;&gt; use model.parameters()
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div> <p>This behaviour constraints <em>A</em> and <em>b</em> to be in the <em>model.parameters()</em> iterator. To do it, we must define them using <em>torch.nn.Parameter</em> (!) instead of <em>torch.Tensor</em>. Hence, a simple definition of a linear layer can be:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleLinearLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>
        <span class="k">return</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
</code></pre></div></div> <p>If you look at some PyTorch implementation, it is more frequent to define parameters as empty tensors and then initialize them in a <em>reset_params</em> functions. To keep it more clear, I have hard coded the initialization.</p> <p>Then, if we want to be sure everything is well initialized, we can inspect parameters using the parameters() iterator:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer</span><span class="o">=</span><span class="n">CustomLayer</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</code></pre></div></div> <p> <br> </p> <h1 id="how-to-create-a-model-">How to create a model ?</h1> <h2 id="layers">Layers</h2> <p>Defining a model requires to create layers and to add an order between them. To begin with, we need to subclass the <em>Module</em> class. It is the base class for all neural networks modules. Then, we need to define layers in the <em>__init__</em> method. There are plenty of predifined layers in torch grouped in several categories: convolutions, pooling, transformer, etc. When looking at the source code, many layers are implemented using their counterpart function present in <em>torch.nn.functional</em>. The module and the sub-module works hand in hand.</p> <p>On a more high-level, we can also grouped layers into two different categories: lazy modules and explicit modules. When defining a model, in Tensorflow, we only have to specify the output shaped and input shapes are inferred automatically. In Torch, it was not the case before lazy modules were implemented.</p> <p><strong>Personal think:</strong></p> <ul> <li>Lazy module: From my point of view, even if it is a nice TensorFlow feature, it is not time consuming to manually calculate input shapes and it ensures we understand how our model works at every steps.</li> </ul> <h2 id="containers">Containers</h2> <p>Once layers are created, we need to order them. The best way to do it depends on the architecture we have. We can use:</p> <ul> <li>nn.Sequential: If layers are sequentially executed.</li> <li>nn.ModuleList or nn.ModuleDict: If we want to register layers but define the call logic later.</li> <li>nn.ParameterList or nn.ParameterDict: If instead of layers we work directly with their parameters.</li> </ul> <p>ModuleDict and ParameterDict are really close to their list counterpart but have a better representation. Sequential does not have a dictionary-like version, however if we want to customize its representation, there is a trick using OrderedDict:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining layers
</span><span class="n">lin_in</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">inter_features</span><span class="p">)</span>
<span class="n">lin_out</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inter_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

<span class="c1"># Order them
</span><span class="n">layers</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s">'lin_in'</span><span class="p">,</span> <span class="n">lin_in</span><span class="p">),</span>
                                  <span class="p">(</span><span class="s">'lin_out'</span><span class="p">,</span> <span class="n">lin_out</span><span class="p">)]))</span>
</code></pre></div></div> <p>As already mentioned, ParameterDict is suited when we work directly with parameters. For instance, we can re-write the previous simple linear layer with the following syntax:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleLinearLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">ParameterDict</span><span class="p">({</span><span class="s">'weight'</span><span class="p">:</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)),</span>
                                     <span class="s">'bias'</span><span class="p">:</span><span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">out_features</span><span class="p">))})</span>
        <span class="k">return</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'weight'</span><span class="p">].</span><span class="n">T</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'bias'</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Questions:</strong></p> <ul> <li>ModuleList vs list of modules: One can wonder why we should use ModuleList instead of a list of layers. One reason is when we define a list of layers, calling <em>model.parameters()</em> does not look inside each item parameters. That is a huge problem because the optimizer is usually defined calling <em>model.parameters()</em> ! It can lead to bad <a href="https://discuss.pytorch.org/t/whats-the-difference-between-nn-modulelist-and-python-list/106401" target="_blank" rel="noopener noreferrer">optimization</a>.</li> </ul> <p> <br> </p> <h1 id="how-to-deploy-a-model-">How to deploy a model ?</h1> <p>Once the model is created, we need to deploy it on one or some machine. If we have a single GPU, we simply have to export the model to the machine using <em>model.to()</em>. This method modifies the module in-place. If we have multiple GPUs, we can not export the model on a single device, that is why <em>DataParallel</em> layers have been created.</p> <p> <br> </p> <h1 id="how-to-inspect-a-model-">How to inspect a model ?</h1> <h2 id="hooks">Hooks</h2> <p>Once the model is created we might want to inspect it to ensure it behaves as expected or to modify its behaviour. That is a typical hook usecase. Hooks are functions that automatically execute after or before a particular event, for instance a forward or a backward call. They can be used for additional prints, catching intermediate results, cliping gradients, applying <a href="https://burhan-mudassar.netlify.app/post/power-of-hooks-in-pytorch/" target="_blank" rel="noopener noreferrer">dropout</a>, <a href="https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904" target="_blank" rel="noopener noreferrer">etc</a>.</p> <p><strong>Example:</strong> To illustrate hooks, we will print intermediate input shapes before every forward. For this purpose, we can assign a hook function for every submodule. This solution requires changing many code and it is not really flexible. A better option is to recursively apply hooks to each layer of our model using a function (apply) or an iterator (with: named_modules or named_children).</p> <p>First we create a model containing nested nn.Module dependencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Imports:
</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="c1"># Classes
</span><span class="k">class</span> <span class="nc">SimpleMLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">OrderedDict</span><span class="p">([</span>
            <span class="p">(</span><span class="s">'lin_in'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">)),</span>
            <span class="p">(</span><span class="s">'lin_out'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">))])</span>
            <span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>Then, we select the type of hook we want and we define the function we should apply:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VerboseModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'named_modules'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span> 
        
        <span class="c1"># Hook before every forward(): register_forward_pre_hook
</span>        <span class="n">match</span> <span class="n">mode</span><span class="p">:</span>
            <span class="n">case</span> <span class="s">'named_modules'</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">named_modules</span><span class="p">():</span>
                    <span class="n">module</span><span class="p">.</span><span class="n">__name__</span><span class="o">=</span><span class="n">name</span>
                    <span class="n">module</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">inspect_shape_in</span><span class="p">)</span>
            <span class="n">case</span> <span class="s">'named_children'</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">named_children</span><span class="p">():</span>
                    <span class="n">module</span><span class="p">.</span><span class="n">__name__</span><span class="o">=</span><span class="n">name</span>
                    <span class="n">module</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">inspect_shape_in</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Hook
</span><span class="k">def</span> <span class="nf">inspect_shape_in</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s">'__name__'</span><span class="p">):</span>
        <span class="n">name</span><span class="o">=</span><span class="n">module</span><span class="p">.</span><span class="n">__name__</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">name</span><span class="o">=</span><span class="s">''</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Model {:15} Input shape {:10}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> 
                                                 <span class="nb">str</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">))</span> <span class="p">))</span>
</code></pre></div></div> <p>Then, we apply hook:</p> <ul> <li>recursively to every submodule (as returned by .children()) as well as self.</li> <li>to every modules (as returned by .named_modules()).</li> <li>to every immediate children (as returned by .named_children())</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span>
    <span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="n">in_f</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'apply'</span><span class="p">,</span> <span class="s">'named_modules'</span><span class="p">,</span> <span class="s">'named_children'</span><span class="p">]:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Mode: {:&lt;15}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
        <span class="n">model</span><span class="o">=</span><span class="n">SimpleMLP</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">)</span>
        
        <span class="n">match</span> <span class="n">mode</span><span class="p">:</span>
            <span class="n">case</span> <span class="s">'apply'</span><span class="p">:</span>
                <span class="c1"># Recursively implicit
</span>                <span class="n">model</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">m</span><span class="p">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">inspect_shape_in</span><span class="p">)</span> <span class="p">)</span>
                <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                
            <span class="n">case</span> <span class="s">'named_modules'</span><span class="o">|</span><span class="s">'named_children'</span><span class="p">:</span>
                <span class="c1"># Recursively explicit
</span>                <span class="n">verbose_model</span><span class="o">=</span><span class="n">VerboseModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
                <span class="n">verbose_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div> <p>Among the three options, the one using <em>apply()</em> is the less flexible but requires less code. Compared to the two other options, it is harder to know what layer we are looking at since we can not directly get their name.</p> <p>The difference between <em>named_modules()</em> and <em>named_children()</em> is that the first is more exaustive than the other and allows to have a deep look into every submodules. In our previous example, <em>named_children()</em> looks only into <em>model.layers</em> while <em>named_modules()</em> looks into <em>model</em>, <em>model.layers</em>, <em>model.layers.lin_in</em>, <em>model.layers.lin_out</em></p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Loick Chambon. Created using the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>