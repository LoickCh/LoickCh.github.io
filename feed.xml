<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://loickch.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://loickch.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-08T13:11:38+00:00</updated><id>https://loickch.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Kaggle TPS.Nov.2022</title><link href="https://loickch.github.io/blog/2023/challenge/" rel="alternate" type="text/html" title="Kaggle TPS.Nov.2022"/><published>2023-01-02T00:00:00+00:00</published><updated>2023-01-02T00:00:00+00:00</updated><id>https://loickch.github.io/blog/2023/challenge</id><content type="html" xml:base="https://loickch.github.io/blog/2023/challenge/"><![CDATA[<h1 id="tps-nov-2022">TPS Nov-2022</h1> <p>Hello, in this note I explain my personnal solution to the TPS challenge where I ranked 18/689 (2.6 %).</p> <p> <br/> </p> <h3 id="table-of-content">Table of content</h3> <ul> <li>Subject</li> <li>Approach</li> <li>Other solutions</li> <li>Conclusion</li> </ul> <p> <br/> </p> <h2 id="1subject">1.Subject</h2> <p>In this competition, there is a collection of predictions and ground truth labels for a binary classification task we do not know about. The objective is to use blending techniques to improve the model’s predictions by combining submissions. Stacking and blending are machine learning techniques learning a meta-model:</p> <ul> <li>stacking: trains a meta-model from a pool of base models. The base models are trained on a complete training set, then the meta-model is trained on the features that are outputs of the base models. This operation can be repeated several times, at the risk of overfitting and having a more complex model.</li> <li>blending: trains a meta-model on a separate holdout set. It is very similar to stacking however, it does not use the same data.</li> </ul> <p>About the data format, submission files are divided in two parts. In the submissions folder, the first half of the rows contains training set predictions. Their name corresponds to the log-loss over the training set. The other half contains predictions for the testing set. The testing set consists of 20,000 rows, and the submission expects probabilities.</p> <p> <br/> </p> <h2 id="2-approach">2. Approach</h2> <p>I am inspired by several notebooks and ideas from other participants. In this competition, exploratory data augmentation was not crucial so I did not focus on it. However, preparing data and chose a good training pipeline helps me to achieve my results.</p> <h3 id="21-pre-processing">2.1 Pre-processing</h3> <h4 id="211-data-format">2.1.1 Data-format</h4> <p>Instead of using a folder of submission files, we concatenate all of them in a single dataframe object containing 5k columns and 40k rows where each column is a submission and each row is an index. Then, to accelerate the loading, we convert the concatenated dataset to a binary format. In our case we select the feather (.ft) extension, but many exists: pickle (specific to Python), parquet, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_feather</span><span class="p">(</span><span class="sh">'</span><span class="s">../input/tps2022novfeather/train.ftr</span><span class="sh">'</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">../input/tps2022novfeather/train_labels.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Once loaded, we observed that some inputs are above 1 or below 0. Since it is supposed to contain probabilities, we clip values between 1e-7 (close to minimum float32 precision) and 1-1e-7 to ensure we manipulate probabilities. Alternatively, we could have flag set a value for outliers (for instance -1 below and 2 above) and use randomforest-like algorithms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Clip data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">EPS</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">EPS</span><span class="p">)</span>
</code></pre></div></div> <p>After that, we split our dataset in training and testing datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Copy and split datasets in two sets.
</span><span class="n">X</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">TRAIN_SIZE</span><span class="p">,:]),</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">TRAIN_SIZE</span><span class="p">:,:])</span>
<span class="n">y</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">TRAIN_SIZE</span><span class="p">]),</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">TRAIN_SIZE</span><span class="p">:])</span>
</code></pre></div></div> <p>Then, before to train models, we modify our datasets according to a bag of tricks coming from diverse sources.</p> <h4 id="212-bag-of-tricks">2.1.2 Bag of tricks</h4> <p>Since the submissions contain binary probabilities, we can consider whether it would be better to invert some of these probabilities in order to improve the log-loss score. We found that 9 elements had better log-loss scores when their probabilities were inverted, so we made this change.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">better_invert_proba_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><span class="nf">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">EPS</span><span class="p">)</span> <span class="o">-</span> <span class="nf">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">EPS</span><span class="p">))</span> 
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
        <span class="p">]</span>
        <span class="p">)</span><span class="o">&gt;</span> <span class="mi">0</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">better_invert_proba_idx</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">better_invert_proba_idx</span><span class="p">,:]</span>
</code></pre></div></div> <p>Then, we used logits instead of probabilities, even though it is generally not necessary or advisable to convert predicted probabilities into logits before solving a binary classification problem. This step is optional, but allows for better results when learning logistic regression according to <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/364013">this</a>.</p> <p>I only kept these two ideas in my final submission but many have been tested and discussed by the community. For instance, considering bad classifier are nearly random classifiers, they have a ROC-AUC scores close to 0.5. Other pre-processing strategies tested were to calibrate the probabilities using isotonic regression or an embedding of the inputs with a neural network.</p> <h3 id="22-model">2.2 Model</h3> <p>Like many other candidates, I used LightAutoML, a Python library performing automated machine learning with high GPU compatibility. There are several other automated machine learning libraries available, such as H2O, PyCaret, TPOT, and Autosklearn. Of these, only <a href="https://github.com/sb-ai-lab/LightAutoML">LightAutoML</a> and <a href="https://github.com/EpistasisLab/tpot">TPOT</a> produced good results for me. I didn’t spend much time on tuning the hyperparameters, so it’s possible that some of the other libraries may perform just as well or better if given more time and attention.</p> <h4 id="221-feature-selection">2.2.1 Feature selection</h4> <p>Feature selection is an important step in building a model. Reducing the number of features often reduces overfitting and speeds up training, especially with 5,000 columns. Ideally, the feature selection method should be independent of the model and should discriminate the data by itself. For instance, we can select features having a high variance or remove features highly correlated to ensure diversity and erase redundancy (for instance using hierarchical clustering on the Spearman rank-order correlations or the Jensen Shanon distance, <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/368905">cf</a>).</p> <p>Another approach consists on training a model to predict labels and then retrieve the most important features for predictions. It is even more effective when training algorithms with L1 or L2 regularisations, as they penalise the model useless model weights. Alternatively to feature selection, we could have applied dimensionality reduction techniques such as Principal Component Analysis or Partial Least Square Regression. It is another way to reduce the shape of our input data. In this competition, PCA, or neural network encoding do not lead to the best results, so I did not keep them.</p> <p>Since performing feature selection by training gave me the best results, I chose it even though it is more prone to overfitting. I tried two different libraries: scikit-learn (in combination with the catboost and LinearSVC algorithms) and LightAutoML set with few parameters. The final submission uses LightAutoML and more specifically the <code class="language-plaintext highlighter-rouge">TabularAutoML</code> class. My method is rather similar to the one developped by <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/363483">A.Ryzkhov</a>. First of all, we need to specify a task. The subject of the challenge is a blending problem, but seen from another angle, it is nothing more or less than a binary classification where each column is a submission. Then, we chose the selection_params:</p> <ul> <li>importance_type: we have the choice between permutation (calls <code class="language-plaintext highlighter-rouge">NpPermutationImportanceEstimator</code>) or gain (calls <code class="language-plaintext highlighter-rouge">ModelBasedImportanceEstimator</code>). Permutation uses a random permutation of elements in a single column for each feature to calculate its significance. Intuitively, if an element is significant, performance deteriorates when it is shuffled. A more advanced techniques compares the importance of features fitted to the target against the importance of features when fitted to noise defined as the shuffled target (<a href="https://www.kaggle.com/code/ogrellier/feature-selection-with-null-importances">cf.</a>).</li> <li>feature_group_size: specify the number of elements permuted during each step of the search. The less it is, the more accurate we are, but the longer it is.</li> <li>select_algos: <code class="language-plaintext highlighter-rouge">linear_l2</code> corresponds to LBFGS L2 regression based on torch while <code class="language-plaintext highlighter-rouge">gbm</code> corresponds to gradient boosting algorithms from LightGBM library. There are several others, but LightGBM is a good starting point because it allows for L1 and L2 regularisation, thus eliminating unnecessary features.</li> <li>mode: specify the feature selection mode between no selection (0), drop zero importances (1) and iterative permutation importances (2).</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">automl</span> <span class="o">=</span> <span class="nc">TabularAutoML</span><span class="p">(</span>
    <span class="n">task</span> <span class="o">=</span> <span class="nc">Task</span><span class="p">(</span><span class="sh">'</span><span class="s">binary</span><span class="sh">'</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="sh">'</span><span class="s">logloss</span><span class="sh">'</span><span class="p">),</span> 
    <span class="n">timeout</span> <span class="o">=</span> <span class="n">TIMEOUT</span><span class="p">,</span>
    <span class="n">cpu_limit</span> <span class="o">=</span> <span class="n">N_THREADS</span><span class="p">,</span>
    <span class="n">selection_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">importance_type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">permutation</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">fit_on_holdout</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">mode</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> 
        <span class="sh">'</span><span class="s">feature_group_size</span><span class="sh">'</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">select_algos</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">linear_l2</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">gbm</span><span class="sh">'</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="n">reader_params</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">n_jobs</span><span class="sh">'</span><span class="p">:</span> <span class="n">N_THREADS</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div> <p>The feature selection takes around 1h on Kaggle.</p> <h4 id="222-training">2.2.2 Training</h4> <p>I have trained and finetuned lightgbm classifiers, catboost classifiers and scikit-learn algorithms (HistGradientBoostingClassifier, LogisticRegression, etc.) using Optuna and even pure deep learning networks, but LightAutoML gave me the best results.</p> <p>The training loop relies again on the <code class="language-plaintext highlighter-rouge">TabularAutoML</code> class with few other hyperparameters:</p> <ul> <li>general_params: specifies which algorithms trained.</li> <li>nn_params: specifies the neural network parameters. The neural architecture is predifined and flagged with <code class="language-plaintext highlighter-rouge">mlp</code>, <code class="language-plaintext highlighter-rouge">dense</code>, <code class="language-plaintext highlighter-rouge">resnet</code> or <code class="language-plaintext highlighter-rouge">denselight</code> but we can custom it changing the activation function (or implementing new ones such as <a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/366518">Growing Cosine Unit</a>, MISH), clipping gradients, select the batch size, etc.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># General neural network parameters
</span><span class="n">general_nn_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">random_state</span><span class="sh">"</span><span class="p">:</span><span class="n">SEED</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">act_fun</span><span class="sh">"</span><span class="p">:</span> <span class="n">Mish</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">n_epochs</span><span class="sh">"</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">bs</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">num_workers</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">path_to_save</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">clip_grad</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">clip_grad_params</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">max_norm</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> 
    <span class="sh">"</span><span class="s">verbose</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">pin_memory</span><span class="sh">"</span><span class="p">:</span><span class="bp">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Main class
</span><span class="n">automl</span> <span class="o">=</span> <span class="nc">TabularAutoML</span><span class="p">(</span>
    <span class="n">task</span> <span class="o">=</span> <span class="nc">Task</span><span class="p">(</span><span class="sh">'</span><span class="s">binary</span><span class="sh">'</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">logloss</span><span class="sh">'</span><span class="p">),</span> 
    <span class="n">cpu_limit</span> <span class="o">=</span> <span class="n">N_THREADS</span><span class="p">,</span>
    <span class="n">general_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">use_algos</span><span class="sh">"</span><span class="p">:</span> <span class="p">[[</span><span class="sh">"</span><span class="s">linear_l2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">mlp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">denselight</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">lgb</span><span class="sh">"</span><span class="p">]],</span> 
    <span class="p">},</span>
    <span class="n">nn_pipeline_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">use_qnt</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span>
    <span class="p">},</span>
    <span class="n">nn_params</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="o">**</span><span class="n">general_nn_params</span><span class="p">},</span>
        <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="o">**</span><span class="n">general_nn_params</span><span class="p">,</span> 
              <span class="o">**</span><span class="p">{</span>
                    <span class="sh">'</span><span class="s">bs</span><span class="sh">'</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
                    <span class="sh">'</span><span class="s">opt_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.04047</span><span class="p">,</span> <span class="sh">'</span><span class="s">weight_decay</span><span class="sh">'</span><span class="p">:</span> <span class="mf">2.43e-05</span><span class="p">},</span> 
                    <span class="sh">'</span><span class="s">clip_grad</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> 
                    <span class="sh">'</span><span class="s">clip_grad_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">max_norm</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0979877026847337</span><span class="p">}</span>
                <span class="p">}</span>
             <span class="p">},</span>
        <span class="sh">"</span><span class="s">2</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="o">**</span><span class="n">general_nn_params</span><span class="p">,</span> 
              <span class="o">**</span><span class="p">{</span>
                    <span class="sh">'</span><span class="s">bs</span><span class="sh">'</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                    <span class="sh">'</span><span class="s">opt_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.00543</span><span class="p">,</span><span class="sh">'</span><span class="s">weight_decay</span><span class="sh">'</span><span class="p">:</span> <span class="mf">2.2282e-05</span><span class="p">},</span>
                    <span class="sh">'</span><span class="s">clip_grad</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> 
                    <span class="sh">'</span><span class="s">clip_grad_params</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">max_norm</span><span class="sh">'</span><span class="p">:</span> <span class="mf">4.683639733744027</span><span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>
    <span class="p">},</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">max_tuning_time</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3600</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">fit_on_holdout</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span>
    <span class="p">},</span>
    <span class="n">reader_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">n_jobs</span><span class="sh">'</span><span class="p">:</span> <span class="n">N_THREADS</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Training
</span><span class="n">X</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span>
<span class="n">oof_pred</span> <span class="o">=</span> <span class="n">automl</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> 
    <span class="n">roles</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">},</span> 
    <span class="n">cv_iter</span><span class="o">=</span><span class="n">train_val_indices</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
</code></pre></div></div> <p>Once the model is trained, we simply predict the probabilities on the test set. If we have many submissions, a last step consists on averaging submissions. A good averaging strategy is to find the optimal weights for each submission on each k-fold sets. It can be done using <code class="language-plaintext highlighter-rouge">scipy.minimize</code> or even applied the whole pipeline previously developed, but the snake bites the tail at the risk of overfitting.</p> <h2 id="3-other-solutions">3. Other solutions</h2> <p>Some winner candidates have shared their solutions:</p> <h3 id="1st-place-solution"><a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/369674">1.st</a> place solution</h3> <p>Keypoints:</p> <ul> <li>Framework: LightAutoML</li> <li>Data: use logits instead of probabilities.</li> <li>Architecture: XGBoost, LightGBM, NNs (with MISH and Swish activation functions).</li> <li>Training: 10 k-Fold cross-validations. It seems to be good to increase the number of folds while doing cross-validation. However, if we increase it too much, we can overfit on our data.</li> <li>Ensembling: uses <a href="https://www.kaggle.com/code/pourchot/stacking-with-scipy-minimize"><code class="language-plaintext highlighter-rouge">scipy.minimize</code></a> to find the optimal weights on the OOF.</li> </ul> <h3 id="3rd-place-solution">3.rd place solution</h3> <p>Keypoints:</p> <ul> <li>Framework: AutoGluon.</li> <li>Feature selection: drop features containing values out of range (0,1) and using Spearman correlation.</li> <li>Calibrations: uses Isotonic Calibration.</li> </ul> <h3 id="7th-place-solution"><a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/369731">7.th</a> place solution</h3> <p>Keypoints:</p> <ul> <li>Feature selection: drop features based on the difference between the loss value of the model with this feature and without it. Performs feature selection with <a href="https://www.kaggle.com/code/ogrellier/feature-selection-with-null-importances">null-importance</a>.</li> </ul> <h2 id="4-conclusion">4. Conclusion</h2> <p>This toy competition was original in a sense it consists on ensembling models to get better performances even if we tackle it as a binary classification task. In fact, we build a meta-model for a task we do not know about. Interestingly, auto machine learning frameworks work well and gave the best results. Many participants have used LightAutoML pushed by simple and effective notebooks written by the authors of the library. In this challenge, the exploratory data analysis was not so important, but having a good feature selection algorithm helped a lot.</p> <hr/> <p>Other sources:</p> <ul> <li><a href="https://medium.com/@stevenyu530_73989/stacking-and-blending-intuitive-explanation-of-advanced-ensemble-methods-46b295da413c">Blending and stacking</a></li> <li><a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/364013">Calibration</a></li> </ul> <p>Interesting techniques not used:</p> <ul> <li><a href="https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/364834">OOF Forward selection</a></li> </ul>]]></content><author><name></name></author><category term="challenge"/><category term="Kaggle"/><summary type="html"><![CDATA[Tabular Playground Series - November 2022]]></summary></entry><entry><title type="html">Torch module explained (torch.autograd)</title><link href="https://loickch.github.io/blog/2022/Torch-autograd/" rel="alternate" type="text/html" title="Torch module explained (torch.autograd)"/><published>2022-10-24T00:00:00+00:00</published><updated>2022-10-24T00:00:00+00:00</updated><id>https://loickch.github.io/blog/2022/Torch-autograd</id><content type="html" xml:base="https://loickch.github.io/blog/2022/Torch-autograd/"><![CDATA[<p>Today we will dive into torch.autograd module, one of them most important under the hood PyTorch’s module. It implements automatic differentiation of arbitrary scalar valued functions.</p> <p> <br/> </p> <h1 id="table-of-contents">Table of contents:</h1> <ul> <li>How to calculate gradients ?</li> <li>How to locally disable gradients ?</li> <li>How to measure performance ?</li> </ul> <p> <br/> </p> <h1 id="how-to-calculate-gradients-">How to calculate gradients ?</h1> <p>Torch.autograd performs automatic differentiation using two different modes: reverse mode and forward mode.</p> <p>The popularity of reverse mode in deep learning comes from the fact that in general the input dimensionality is higher than the output dimensionality. But theorically, for any \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\), the reverse mode should be used if and only if \(n&gt;&gt;m\), otherwise if \(n&lt;&lt;m\), forward mode is more performant.</p> <p>The reason behind is the order we multiply Jacobian matrices in the chain rule. Indeed, considering real function f and g such that \(f: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_1}\) and \(g: \mathbb{R}^{n_1} \rightarrow \mathbb{R}^{n_2}\) where \((n_0,n_1,n_2) \in \mathbb{N}^3\). Then, if we consider the derivative of \(y= g \circ f\) on \(x \in \mathbb{R}^{n_0}\), we have:</p> \[\frac{\partial y}{\partial x}(x) = \frac{\partial y}{\partial g}(g(x)).\frac{\partial g}{\partial f}(f(x)).\frac{\partial f}{\partial x}(x)\] <p>In multi-dimensional space, the analogy of derivatives is the Jacobian matrix defined for any \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\) by:</p> \[J_f= \begin{bmatrix} \frac{\partial f_1}{x_1} ... \frac{\partial f_1}{x_n} \\ ...\\ \frac{\partial f_m}{x_1} ... \frac{\partial f_m}{x_n} \end{bmatrix}\] <p>And the chain rule can be re-written, for any \(f: \mathbb{R}^{n_1} \rightarrow \mathbb{R}^{n_2}\) and \(g: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_1}\) two real functions, as:</p> \[J_{f\circ g} = \underset{n_2 \times n_1}{J_{f}(g)}.\underset{n_1 \times n_0}{J_g}\] <p>This implies we can rewrite the Jacobian matrix of a composition as a product of two matrices. If we have a composition of several functions, we have to calculate a matrix product.</p> <p>The optimised way of calculating the matrix product is not always straightforward, it depends on the dimensions of the matrix. If we multiply matrices from right to left, we perform forward AD, otherwise if we multiply matrices from left to right, we perform backward AD.</p> <h3 id="example">Example</h3> <p>Let us calculate the gradient of a two layer MLP written as functions defined by \(h=f_3 \circ f_2 \circ f_1\) where \(f_1: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_1}\), \(f_2: \mathbb{R}^{n_1} \rightarrow \mathbb{R}^{n_2}\) and \(f_3: \mathbb{R}^{n_2} \rightarrow \mathbb{R}^{n_3}\) such that:</p> \[\forall i \in \{1,2,3\}, \forall x\in \mathbb{R}^{n_{i-1}}, f_i(x)=x.W_i^T + b_i\] <p>Where:</p> \[\forall i \in \{1,2,3\}, W_i \in \mathbb{R}^{n_i \times n_{i-1}}, b_i\in \mathbb{R}^{n_i}\] <p>We have:</p> \[\forall i \in [[1,...n_0]], \frac{\partial f_1}{\partial x_i} = \frac{\partial x.W_{1}^{T}}{\partial x_i} = L_i^{*1}\] <p>Hence:</p> \[J_{f_1}(a) = (L_1^{*1}, ..., L_{n_0}^{*1}) = (C_1^{1}, ..., C_{n_0}^{1}) = W_1 \in \mathbb{R}^{n_1 \times n_0}\] <p>Using the chain rule,</p> \[J_h(a) =J_{f_3 \circ f_2 \circ f_1}(a)= J_{f_3}(f_2 \circ f_1(a)).J_{f_2}(f_1(a)).J_{f_1}(a) = W_3. W_2 . W_1\] <p>So the Jacobian is the product of the three weight matrices. We can calculate it using from left to right product (reverse mode) or from right to left product (forward mode).</p> <h2 id="reverse-mode">Reverse mode</h2> <p>In PyTorch, reverse mode automatic differentation is the by default mode. To calculate gradients, <em>torch.autograd</em> contains two main functions:</p> <ul> <li><em>backward()</em>: convenient when working with a torch.nn model since it does not require the user to specify which Tensors we want the gradient for. It adds a <em>.grad</em> field to the leaf tensors.</li> <li><em>grad()</em>: does not add a grad field and requires to specify the Tensors we want the gradients for.</li> </ul> <h3 id="theory">Theory</h3> <p>To perform reverse AD, PyTorch records a computation graph containing all the operations to go from inputs to outputs. It gives us a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. The graph is created in forward pass, while gradients are calculated in backward pass using the computation graph.</p> <p>One important thing to note is that intermediary results are saved during the forward pass in order to execute the backward pass. Those intermediary tensors can increase GPU memory consumption on training.</p> <p>Let us see an example with a simple linear layer. We use torchviz to visualize execution graphs and traces. This code must be launched in a notebook after having installed graphviz and torchviz.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>graphviz   
pip <span class="nb">install </span>torchviz
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Imports:
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torchviz</span> <span class="kn">import</span> <span class="n">make_dot</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="c1"># Params
</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="mi">2</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="nc">OrderedDict</span><span class="p">([</span> <span class="p">(</span><span class="sh">'</span><span class="s">Lin1</span><span class="sh">'</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">out_f</span><span class="p">))</span> <span class="p">])</span> <span class="p">)</span>

<span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">make_dot</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">params</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">()),</span> <span class="n">show_attrs</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_saved</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="row justify-content-md-center"> <div class="col-sm-4"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/2022-10-24/output-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Blog/2022-10-24/output-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/2022-10-24/output-1400.webp"/> <img class="img-fluid rounded z-depth-1" src="/assets/img/Blog/2022-10-24/output.png"/> </picture> </figure> </div> <div class="col-sm"> <font color="blue"> Blue boxes:</font> represent tensors requiring a gradient computation. We can see the linear weight matrix of shape (2,16) and the linear bias matrix of shape (2). <br/> <br/> <font color="orange"> Orange boxes:</font> represent intermediary tensors. Those are tensors saved during the forward pass that enables to calculate gradients during the backward. <br/> <br/> <font color="grey"> Grey boxes:</font> minimal transformations and traces. Let us explain some boxes: <ul> <li><i> AccumulateGrad </i>: indicates that gradients need to be calculated. Since weight and bias matrices are encoded as nn.Parameter, they have requires_grad set to True and appear in blue while the input has requires_grad set to False and does not appear;</li> <li><i> TBackward0 </i>: refers to the transposed operation (W<sup>T</sup>);</li> <li><i>MmBackward0</i>: refers to the matrix multiplication (x.W<sup>T</sup>);</li> <li><i>AddBackward0</i>: refers to the addition (x.W<sup>T</sup> + b);</li> </ul> </div> </div> <p>Intermediary tensors imply additional memory cost during training. It is one possible reason explaining why a model fit in memory during evaluation and not during training. These intermediary results are packed during the forward pass (saved) and unpack during the backward pass (when access to the tensor is required). If we want to control the packing/ unpacking behaviour, we can use the <em>torch.autograd.graph.saved_tensors_hooks()</em> context manager.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Call every time a tensor is saved: forward pass
</span><span class="k">def</span> <span class="nf">pack_hook</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Packing</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Call every time a tensor is accessed: backward pass
</span><span class="k">def</span> <span class="nf">unpack_hook</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Unpacking</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="nf">saved_tensors_hooks</span><span class="p">(</span><span class="n">pack_hook</span><span class="p">,</span> <span class="n">unpack_hook</span><span class="p">):</span>
    <span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># Call pack_hook and defined unpack_hook
</span><span class="n">y</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># Call unpack_hook
</span></code></pre></div></div> <p>If this feature is useful for debugging and adds modularity but what is more interesting is to control wether the intermediary tensors are saved on the cpu or on the GPU because very often, the tensors involved in the computation graph live on GPU. Keeping a reference to those tensors in the graph is what causes most models to run out of GPU memory during training while they would have done fine during evaluation. If it is possible to do it using <em>pack_hook</em>, <em>unpack_hook</em>, PyTorch provides another context manager <em>torch.autograd.graph.save_on_cpu()</em>. During the forward pass, tensors will be stored on CPU, while during backward pass, they will be put to the GPU.</p> <h3 id="example-1">Example:</h3> <p>We continue the theorical example of the composition of three nested linear layers to verify that the Jacobian of the composition is the product of the weight matrices. To do this, we can either use the <em>torch.autograd.grad()</em> or <em>torch.autograd.backward().</em></p> <p>Let us begin with “backward”. According to the documentation, it computes the sum of gradients with respect to graph leaves. We note that it has an argument called <em>grad_tensors</em> specifying the gradient of the output. It multiplies the result on the left by <em>grad_tensors</em>. If we note it <em>G</em> and \(f\) the function we are calculating the jacobian, then the result will of <em>torch.autograd.backward</em> is: \(G . J_f\)</p> <p>For instance, if \(G\) is equals to a tensor full of one, we have:</p> \[(1, ... , 1) . J_f = \Big(\sum_{i=1}^{m} \frac{\partial f_i}{x_j} \Big)_{j \in \{1,...,n\}}\] <p>In practice,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Imports:
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Main:
</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">2</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="o">=</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">))</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="p">)</span>

<span class="c1"># Forward
</span><span class="n">y</span><span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">grad_output</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">out_f</span><span class="p">)</span>

<span class="c1"># Backward: sum of gradients
</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">grad_tensors</span><span class="o">=</span><span class="n">grad_output</span><span class="p">)</span>
<span class="n">jacob_sum</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span>

<span class="c1"># Sanity check
</span><span class="n">prod</span><span class="o">=</span><span class="n">lin_out</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_inter</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_in</span><span class="p">.</span><span class="n">weight</span> <span class="c1"># W_3.W_2.W_1
</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jacob_sum</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">prod</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># Equality !
</span></code></pre></div></div> <p>The <em>torch.autograd.grad()</em> counterpart is really close to the previous implementation and rather straightforward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Change this
# torch.autograd.backward(y, grad_tensors=grad_output)
# jacob_sum=x.grad
# By
</span><span class="n">jacob_sum</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_output</span><span class="p">)</span>
</code></pre></div></div> <h2 id="forward-mode">Forward mode</h2> <p>PyTorch 1.12 has a beta API implementing forward mode AD. It contains three main functions:</p> <ul> <li>foward_ad.dual_level(): a context manager ensuring that all dual tensors created inside will have their tangents destroyed upon exit. Convenient to avoid confusing tangents from different computations.</li> <li>foward_ad.make_dual(): create a dual tensor associating a primal tensor with a dual tensor. The name comes from the dual number theory.</li> <li>foward_ad.unpack_dual(): unpacks a dual tensor giving its Tensor value and its forward AD gradient.</li> </ul> <h3 id="theory-1">Theory</h3> <p>We have already explained that the theorical difference between reverse and forward mode is the way we compute matrix product. In practice, we can not use the same pipeline, i.e create a computational graph and backpropagate. To implement forward AD, we need a bit of dual number theory.</p> <p>Let us write any dual number \(x=a+b. \epsilon\) where \(\epsilon^2=0\). When we apply, for any real function, the Taylor series on a dual number, we have: \(f(a+b\epsilon)=\sum_{n=0}^{\infty} \frac{f^{(n)}(a) b^n \epsilon^n }{n!} = f(a)+b.f^{'}(a).\epsilon\) since \(\epsilon^2=0\).</p> <p>Hence, for any real function g, we have:</p> \[f(g(x)) = f(g(a) + b.g^{'}(a)\epsilon)= f(g(a)) + b.g^{'}(a).f^{'}(g(a)).\epsilon\] <p>Moreover, we have for any real function f and g:</p> \[f(x).g(x)=(f(a)+b.f^{'}(a).\epsilon).(g(a)+b.g^{'}(a).\epsilon) =f(a).g(a) + (b.f^{'}(a).g(a)+f(a).b.g^{'}(a)).\epsilon\] <p>The previous results are sufficient to calculate derivatives. For instance, if \(f: \mathbb{R} \rightarrow \mathbb{R}\) such that \(\forall x \in \mathbb{R}, f(x)=x^2.sin(x)\). If we want to calculate the derivative on 3, we do the following calculus:</p> \[x=3+1.\epsilon, x^2=9+6.\epsilon, sin(x)=sin(3)+cos(3).\epsilon\] \[f(x)=x^2.sin(x)=(9+6.\epsilon).(sin(3)+cos(3)\epsilon)= 9.sin(3)+ (9.cos(3)+6.sin(3)). \epsilon\] <p>Hence,</p> \[f(3)=9.sin(3) \text{ and } f^{'}(3)=(9.cos(3)+6.sin(3))\] <h3 id="example-2">Example</h3> <p>We use the same example as before and keep the notations of the official introductory notebook: primal (input, previously “x”) and tangent (input derivatives). Using the same notations as before, tangent <em>T</em> applies the following transformation on the Jacobian: \(J_f . T\)</p> <p>Thus, if \(T\) is equals to a tensor full of one, we have:</p> \[J_f . \begin{pmatrix} 1 \\ ... \\ 1 \end{pmatrix} = \Big(\sum_{j=1}^{n} \frac{\partial f_j}{x_i} \Big)_{i \in \{1,...,m\}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Params
</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">2</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tangent</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">in_f</span><span class="p">)</span>
<span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="o">=</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">))</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="p">)</span>

<span class="c1"># Forward
</span><span class="k">with</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">dual_level</span><span class="p">():</span>
    <span class="n">dual_input</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">make_dual</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">dual_input</span><span class="p">)</span>
    <span class="n">jvp</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">unpack_dual</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">tangent</span>

<span class="c1"># Sanity check
</span><span class="n">prod</span><span class="o">=</span><span class="n">lin_out</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_inter</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_in</span><span class="p">.</span><span class="n">weight</span> <span class="c1"># W_3.W
</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jvp</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">prod</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># Equality !
</span></code></pre></div></div> <p>The result is the same as before !</p> <p> <br/> </p> <h2 id="reverse-ad-vs-forward-ad">Reverse AD vs forward AD</h2> <p>In the introduction, I presented the interest of the AD forward method when the dimensionality of the output space is higher than that of the input space. It is time to verify the statement in coding.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="mi">16</span>

<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">)</span>

<span class="c1"># Forward AD
</span><span class="n">primal</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
<span class="n">tangents</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">jacob_fwd</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="n">tangents</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">dual_level</span><span class="p">():</span>
        <span class="n">dual_input</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">make_dual</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">dual_input</span><span class="p">)</span>
        <span class="n">jvp</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">unpack_dual</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">tangent</span>
    <span class="n">jacob_fwd</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">jvp</span><span class="p">)</span>
<span class="n">jacob_fwd</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">jacob_fwd</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Forward AD: </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="o">&gt;</span><span class="mf">21.5</span><span class="n">f</span><span class="si">}</span><span class="s">s shape </span><span class="si">{</span><span class="n">jvp</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Reverse AD
</span><span class="n">inp</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">gradients</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="n">jacob_rev</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">grad_tensors</span><span class="o">=</span><span class="n">grad</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">jacob_rev</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">inp</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">jacob_rev</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">jacob_rev</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Backward AD: </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="o">&gt;</span><span class="mf">20.5</span><span class="n">f</span><span class="si">}</span><span class="s">s shape </span><span class="si">{</span><span class="n">inp</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jacob_rev</span><span class="p">,</span> <span class="n">jacob_fwd</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div> <p>One could use more advanced functions to criticise the use of ‘for’ loops. However, this code gives us a first impression of the time difference and is not intended to be as optimised as possible. Nonetheless, results are striking. The difference in computation time between the two approaches is of several magnitudes depending on whether the final dimension is greater than the initial one.</p> <p>This result is very important because if I had said that the popularity of reverse AD in deep learning was due to the fact that the output dimension is usually smaller than the input dimension, there are problems and architectures (encoder) that do not verify this result.</p> <p>One can hope that one day the optimisation will intelligently choose which mode to use according to the intermediate spaces of the architecture.</p> <h2 id="optimized-comparison">Optimized comparison</h2> <p>The last code snippet, give us a hint about the best mode depending on the situation. However, it was a bit tedious. That is why, PyTorch has implemented optimized functions in <em>torch.autograd.functional</em> to calculate Jacobians (1st order) and Hessians (2nd order) in a beta API. PyTorch is trying to catch up with JAX on the optimized calculation of high-order gradients.</p> <h3 id="example-3">Example:</h3> <p>For instance, instead of using a for loop to calculate jacobian, we can use a one-line function. The results are faster but the conclusion is the same: the time difference depends on the distance between the dimensions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="mi">1024</span>

<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">)</span>

<span class="c1"># Forward 
</span><span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">jacob_fwd</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">vectorize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">forward-mode</span><span class="sh">'</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Time: {:10.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>

<span class="c1"># Reverse
</span><span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">jacob_rev</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Time: {:10.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jacob_rev</span><span class="p">,</span> <span class="n">jacob_fwd</span><span class="p">)</span>
</code></pre></div></div> <p> <br/> </p> <h1 id="how-to-locally-disable-gradients-">How to locally disable gradients ?</h1> <p>There are two common situations where we want to disable gradients: during inference or using custom code. Different context manager help us to globally disable or enable gradient calculation and storage. There are three different context:</p> <ul> <li>grad mode: it is the by default mode and is the only mode in which <em>requires_grad</em> takes effect. It is used when training models.</li> <li>no-grad mode (<em>torch.set_grad_enabled(False)</em> or <em>torch.no_grad()</em>): convenient to temporary disable the tracking of any operations requires to latter calculate gradients without having to set <em>requires_grad</em> to False and then back to True.</li> <li>inference mode (<em>torch.inference_mode()</em>): optimized version of no-grad mode disabling view tracking and version counter bumps. It is used in during data processing and evaluation.</li> </ul> <p><strong>Note:</strong></p> <ul> <li>model.eval() is not a context manager. It only change the behaviour of modules acting differently during training and evaluation (nn.Dropout, nn.BatchNorm2d)</li> <li>None of the context manager is available for forward AD.</li> </ul> <p> <br/> </p> <h1 id="how-to-measure-performance-">How to measure performance ?</h1> <h2 id="profiling">Profiling</h2> <p>Profiling code is really important to optimize code, find bottlenecks, check GPU memory, check CPU memory, etc. PyTorch has implemented a profiler recording GPU and CPU events with a simple context manager <em>torch.autograd.profiler.profile()</em>. Originally it was part of <em>torch.autograd</em> module since it deals with code optimisation. But, with PyTorch 1.8.1 release, the module is considered legacy and is deprecated in favour of <em>torch.profiler</em> module.</p> <p>Autograd profiler has many interesting arguments such as:</p> <ul> <li>enabled: allows to enable or not profiling. It is useful so that we do not have to comment on or uncomment the code, whether we want to profile it or not.</li> <li>use_cuda: enables timing of CUDA event.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="mi">1</span>
<span class="n">bool_profile</span><span class="o">=</span><span class="bp">False</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">out_f</span><span class="p">))</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">profiler</span><span class="p">.</span><span class="nf">profile</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">bool_profile</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>       

<span class="k">if</span> <span class="n">prof</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Save profiling
</span>    <span class="n">prof</span><span class="p">.</span><span class="nf">export_chrome_trace</span><span class="p">(</span><span class="sh">'</span><span class="s">result_profiling</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Print table order by CPU time 
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">prof</span><span class="p">.</span><span class="nf">key_averages</span><span class="p">().</span><span class="nf">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="sh">"</span><span class="s">self_cpu_time_total</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div> <p>After profiling, we can print table in the terminal directly using a simple print or after having grouped functions using <em>profiler.profile.key_averages</em>. Another intersting tools is to save a trace of the profiling and to load it later in: <em>chrome://tracing</em></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/2022-10-24/tracing-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Blog/2022-10-24/tracing-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/2022-10-24/tracing-1400.webp"/> <img class="img-fluid rounded z-depth-1" src="/assets/img/Blog/2022-10-24/tracing.png" title="chrome://tracing visualisation"/> </picture> </figure> </div> </div> <div class="caption"> chrome://tracing visualisation </div> <hr/> <h1 id="ressources">Ressources</h1> <p><strong>Concepts:</strong></p> <ul> <li><a href="https://pytorch.org/docs/stable/autograd.html#">AD</a></li> <li><a href="https://towardsdatascience.com/forward-mode-automatic-differentiation-dual-numbers-8f47351064bf">Understand AD</a></li> <li><a href="https://math.stackexchange.com/questions/2195377/reverse-mode-differentiation-vs-forward-mode-differentiation-where-are-the-be">Understand AD (2)</a></li> <li><a href="https://jingnanshi.com/blog/autodiff.html">Mathematical aspects of AD</a></li> </ul> <p><strong>Benchmark:</strong></p> <ul> <li><a href="https://leimao.github.io/article/Automatic-Differentiation/">Comparison between AD modes</a></li> <li><a href="https://leimao.github.io/blog/PyTorch-Automatic-Differentiation/">AD mode time comparison</a></li> </ul> <p><strong>Notebooks:</strong></p> <ul> <li><a href="https://pytorch.org/tutorials/intermediate/forward_ad_usage.html">AD</a></li> <li><a href="https://pytorch.org/docs/stable/notes/autograd.html#">Autograd</a></li> <li><a href="https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html">Autograd Hooks</a></li> </ul> <p><em>News:</em></p> <ul> <li><a href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/">Profiler</a></li> </ul>]]></content><author><name></name></author><category term="library"/><category term="python"/><summary type="html"><![CDATA[Exploring and explaining torch functionalities.]]></summary></entry><entry><title type="html">Torch module explained (torch.nn)</title><link href="https://loickch.github.io/blog/2022/Torch-nn/" rel="alternate" type="text/html" title="Torch module explained (torch.nn)"/><published>2022-10-23T00:00:00+00:00</published><updated>2022-10-23T00:00:00+00:00</updated><id>https://loickch.github.io/blog/2022/Torch-nn</id><content type="html" xml:base="https://loickch.github.io/blog/2022/Torch-nn/"><![CDATA[<p>Today we will dive in torch.nn module, one of them most important Pytorch’s module to create neural networks and to customise existing code.</p> <p> <br/> </p> <h1 id="table-of-contents">Table of contents:</h1> <p>Torch.nn introduces new concept and tries to answer the following questions:</p> <ul> <li>Notions</li> <li>How to create a model ?</li> <li>How to deploy a model ?</li> <li>How to inspect a model ?</li> </ul> <p> <br/> </p> <h1 id="notions">Notions</h1> <h2 id="subclass-nnparameter">Subclass: nn.Parameter</h2> <p><strong>Sum up:</strong> nn.Parameter is a subclass of torch.Tensor used when we need to optimize tensors during the gradient descent. It automatically add tensors to the parameters() iterator allowing us to simply define an optimizer.</p> <p><strong>Example:</strong> To illustrate the notion, let us implement a linear layer. To recall, a linear layer modifies the number of channels of an input tensor <em>x</em> applying a single hidden layer. It is defined with a weight matrix <em>A</em> and bias matrix <em>b</em> by the following equation: \(y=x.A^T + b\) Where y has shape $(<em>,H_{out})$ and x has shape $(</em>,H_{in})$.</p> <p>Thus, to implement the layer, we need to define <em>A</em> and <em>b</em>, i.e to initialize a weight and a bias matrix and to specify that they need to be updated during optimization. A common practice to define an optimizer is to call <em>model.parameters()</em> returning an iterator with all model registered parameters:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SGD optimizer
# &gt;&gt; use model.parameters()
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div> <p>This behaviour constraints <em>A</em> and <em>b</em> to be in the <em>model.parameters()</em> iterator. To do it, we must define them using <em>torch.nn.Parameter</em> (!) instead of <em>torch.Tensor</em>. Hence, a simple definition of a linear layer can be:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleLinearLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>
        <span class="k">return</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>
</code></pre></div></div> <p>If you look at some PyTorch implementation, it is more frequent to define parameters as empty tensors and then initialize them in a <em>reset_params</em> functions. To keep it more clear, I have hard coded the initialization.</p> <p>Then, if we want to be sure everything is well initialized, we can inspect parameters using the parameters() iterator:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer</span><span class="o">=</span><span class="nc">CustomLayer</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</code></pre></div></div> <p> <br/> </p> <h1 id="how-to-create-a-model-">How to create a model ?</h1> <h2 id="layers">Layers</h2> <p>Defining a model requires to create layers and to add an order between them. To begin with, we need to subclass the <em>Module</em> class. It is the base class for all neural networks modules. Then, we need to define layers in the <em>__init__</em> method. There are plenty of predifined layers in torch grouped in several categories: convolutions, pooling, transformer, etc. When looking at the source code, many layers are implemented using their counterpart function present in <em>torch.nn.functional</em>. The module and the sub-module works hand in hand.</p> <p>On a more high-level, we can also grouped layers into two different categories: lazy modules and explicit modules. When defining a model, in Tensorflow, we only have to specify the output shaped and input shapes are inferred automatically. In Torch, it was not the case before lazy modules were implemented.</p> <p><strong>Personal think:</strong></p> <ul> <li>Lazy module: From my point of view, even if it is a nice TensorFlow feature, it is not time consuming to manually calculate input shapes and it ensures we understand how our model works at every steps.</li> </ul> <h2 id="containers">Containers</h2> <p>Once layers are created, we need to order them. The best way to do it depends on the architecture we have. We can use:</p> <ul> <li>nn.Sequential: If layers are sequentially executed.</li> <li>nn.ModuleList or nn.ModuleDict: If we want to register layers but define the call logic later.</li> <li>nn.ParameterList or nn.ParameterDict: If instead of layers we work directly with their parameters.</li> </ul> <p>ModuleDict and ParameterDict are really close to their list counterpart but have a better representation. Sequential does not have a dictionary-like version, however if we want to customize its representation, there is a trick using OrderedDict:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining layers
</span><span class="n">lin_in</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">inter_features</span><span class="p">)</span>
<span class="n">lin_out</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

<span class="c1"># Order them
</span><span class="n">layers</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="nc">OrderedDict</span><span class="p">([(</span><span class="sh">'</span><span class="s">lin_in</span><span class="sh">'</span><span class="p">,</span> <span class="n">lin_in</span><span class="p">),</span>
                                  <span class="p">(</span><span class="sh">'</span><span class="s">lin_out</span><span class="sh">'</span><span class="p">,</span> <span class="n">lin_out</span><span class="p">)]))</span>
</code></pre></div></div> <p>As already mentioned, ParameterDict is suited when we work directly with parameters. For instance, we can re-write the previous simple linear layer with the following syntax:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleLinearLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">ParameterDict</span><span class="p">({</span><span class="sh">'</span><span class="s">weight</span><span class="sh">'</span><span class="p">:</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)),</span>
                                     <span class="sh">'</span><span class="s">bias</span><span class="sh">'</span><span class="p">:</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">out_features</span><span class="p">))})</span>
        <span class="k">return</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">weight</span><span class="sh">'</span><span class="p">].</span><span class="n">T</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">bias</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Questions:</strong></p> <ul> <li>ModuleList vs list of modules: One can wonder why we should use ModuleList instead of a list of layers. One reason is when we define a list of layers, calling <em>model.parameters()</em> does not look inside each item parameters. That is a huge problem because the optimizer is usually defined calling <em>model.parameters()</em> ! It can lead to bad <a href="https://discuss.pytorch.org/t/whats-the-difference-between-nn-modulelist-and-python-list/106401">optimization</a>.</li> </ul> <p> <br/> </p> <h1 id="how-to-deploy-a-model-">How to deploy a model ?</h1> <p>Once the model is created, we need to deploy it on one or some machine. If we have a single GPU, we simply have to export the model to the machine using <em>model.to()</em>. This method modifies the module in-place. If we have multiple GPUs, we can not export the model on a single device, that is why <em>DataParallel</em> layers have been created.</p> <p> <br/> </p> <h1 id="how-to-inspect-a-model-">How to inspect a model ?</h1> <h2 id="hooks">Hooks</h2> <p>Once the model is created we might want to inspect it to ensure it behaves as expected or to modify its behaviour. That is a typical hook usecase. Hooks are functions that automatically execute after or before a particular event, for instance a forward or a backward call. They can be used for additional prints, catching intermediate results, cliping gradients, applying <a href="https://burhan-mudassar.netlify.app/post/power-of-hooks-in-pytorch/">dropout</a>, <a href="https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904">etc</a>.</p> <p><strong>Example:</strong> To illustrate hooks, we will print intermediate input shapes before every forward. For this purpose, we can assign a hook function for every submodule. This solution requires changing many code and it is not really flexible. A better option is to recursively apply hooks to each layer of our model using a function (apply) or an iterator (with: named_modules or named_children).</p> <p>First we create a model containing nested nn.Module dependencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Imports:
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="c1"># Classes
</span><span class="k">class</span> <span class="nc">SimpleMLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">OrderedDict</span><span class="p">([</span>
            <span class="p">(</span><span class="sh">'</span><span class="s">lin_in</span><span class="sh">'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">)),</span>
            <span class="p">(</span><span class="sh">'</span><span class="s">lin_out</span><span class="sh">'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">))])</span>
            <span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>Then, we select the type of hook we want and we define the function we should apply:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VerboseModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">named_modules</span><span class="sh">'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span> 
        
        <span class="c1"># Hook before every forward(): register_forward_pre_hook
</span>        <span class="n">match</span> <span class="n">mode</span><span class="p">:</span>
            <span class="n">case</span> <span class="sh">'</span><span class="s">named_modules</span><span class="sh">'</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
                    <span class="n">module</span><span class="p">.</span><span class="n">__name__</span><span class="o">=</span><span class="n">name</span>
                    <span class="n">module</span><span class="p">.</span><span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="n">inspect_shape_in</span><span class="p">)</span>
            <span class="n">case</span> <span class="sh">'</span><span class="s">named_children</span><span class="sh">'</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
                    <span class="n">module</span><span class="p">.</span><span class="n">__name__</span><span class="o">=</span><span class="n">name</span>
                    <span class="n">module</span><span class="p">.</span><span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="n">inspect_shape_in</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Hook
</span><span class="k">def</span> <span class="nf">inspect_shape_in</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="sh">'</span><span class="s">__name__</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">name</span><span class="o">=</span><span class="n">module</span><span class="p">.</span><span class="n">__name__</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">''</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Model {:15} Input shape {:10}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> 
                                                 <span class="nf">str</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">))</span> <span class="p">))</span>
</code></pre></div></div> <p>Then, we apply hook:</p> <ul> <li>recursively to every submodule (as returned by .children()) as well as self.</li> <li>to every modules (as returned by .named_modules()).</li> <li>to every immediate children (as returned by .named_children())</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span>
    <span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="n">in_f</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">apply</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">named_modules</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">named_children</span><span class="sh">'</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Mode: {:&lt;15}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
        <span class="n">model</span><span class="o">=</span><span class="nc">SimpleMLP</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">)</span>
        
        <span class="n">match</span> <span class="n">mode</span><span class="p">:</span>
            <span class="n">case</span> <span class="sh">'</span><span class="s">apply</span><span class="sh">'</span><span class="p">:</span>
                <span class="c1"># Recursively implicit
</span>                <span class="n">model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">m</span><span class="p">.</span><span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="n">inspect_shape_in</span><span class="p">)</span> <span class="p">)</span>
                <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                
            <span class="n">case</span> <span class="sh">'</span><span class="s">named_modules</span><span class="sh">'</span><span class="o">|</span><span class="sh">'</span><span class="s">named_children</span><span class="sh">'</span><span class="p">:</span>
                <span class="c1"># Recursively explicit
</span>                <span class="n">verbose_model</span><span class="o">=</span><span class="nc">VerboseModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
                <span class="nf">verbose_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Among the three options, the one using <em>apply()</em> is the less flexible but requires less code. Compared to the two other options, it is harder to know what layer we are looking at since we can not directly get their name.</p> <p>The difference between <em>named_modules()</em> and <em>named_children()</em> is that the first is more exaustive than the other and allows to have a deep look into every submodules. In our previous example, <em>named_children()</em> looks only into <em>model.layers</em> while <em>named_modules()</em> looks into <em>model</em>, <em>model.layers</em>, <em>model.layers.lin_in</em>, <em>model.layers.lin_out</em></p>]]></content><author><name></name></author><category term="library"/><category term="python"/><summary type="html"><![CDATA[Exploring and explaining torch functionalities.]]></summary></entry><entry><title type="html">HackerRank (Python)</title><link href="https://loickch.github.io/blog/2022/hackerrank-python/" rel="alternate" type="text/html" title="HackerRank (Python)"/><published>2022-10-03T00:00:00+00:00</published><updated>2022-10-03T00:00:00+00:00</updated><id>https://loickch.github.io/blog/2022/hackerrank-python</id><content type="html" xml:base="https://loickch.github.io/blog/2022/hackerrank-python/"><![CDATA[<h3 id="progress">Progress</h3> <ul> <li>Actual rank: 11.377</li> <li>Problem solved: 95%</li> </ul> <p> <br/> </p> <h1 id="collections">Collections</h1> <ul> <li>deque:</li> </ul> <p>In the official Python documentation, collections.deque() is defined as a list-like container with fast appends and pops on either end. It has an approximate O(1) complexity for such operations. It supports a <em>maxlen</em> argument. If specify, once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end.</p> <p>It has many useful methods such as: <em>extendleft(), appendleft(), popleft(), remove(), rotate()</em>, etc.</p> <p> <br/> </p> <h1 id="basic-data-types">Basic data types</h1> <ul> <li>eval():</li> </ul> <p>According to <a href="https://realpython.com/python-eval-function/"> Real Python</a>, Python’s eval() allows you to evaluate arbitrary Python expressions from a string-based or compiled-code-based input.</p> <p>It is used in the <a href="https://www.hackerrank.com/challenges/python-lists/submissions/code/293800673">Lists </a> challenge to evaluate different list-methods such as <em>.insert</em>, <em>.remove</em>, <em>.pop</em>, <em>.reverse</em>, etc. A possible answer is the following code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
    <span class="n">List</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">command</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="o">=</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">command</span> <span class="o">!=</span> <span class="sh">'</span><span class="s">print</span><span class="sh">'</span><span class="p">:</span>
            <span class="nf">eval</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">List.</span><span class="si">{</span><span class="n">command</span><span class="si">}</span><span class="s">(</span><span class="si">{</span><span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">List</span><span class="p">)</span>
</code></pre></div></div> <p><em>Tips:</em> It is intersting to notice how we combined f-strings with personalized command and string-to-list conversion to add arguments.</p> <p><em>Warning:</em> eval() must always be used passing globals and locals dictionnaries specifying which expressions can or can not be used to prevent a user from bad manipulations such as <em>os.system(‘rm -rf *’)</em>. For more details, click on the following &lt;a href=https://www.programiz.com/python-programming/methods/built-in/eval&gt;website&lt;/a&gt;.</p> <p> <br/> </p> <h1 id="itertools">Itertools</h1> <ul> <li>groupby():</li> </ul> <p>Quoting from the official Python &lt;a href=https://docs.python.org/3/library/itertools.html#itertools.groupby&gt; documentation &lt;/a&gt;, <em>groupby()</em> is used to:</p> <p>“Make an iterator that returns consecutive keys and groups from the iterable. The key is a function computing a key value for each element. If not specified or is None, key defaults to an identity function and returns the element unchanged. Generally, the iterable needs to already be sorted on the same key function.</p> <p>The operation of groupby() is similar to the uniq filter in Unix. It generates a break or new group every time the value of the key function changes (which is why it is usually necessary to have sorted the data using the same key function). That behavior differs from SQL’s GROUP BY which aggregates common elements regardless of their input order.”</p> <p>In the “Compress the string challenge”, we have to group the consecutive occurences of string characters. One possible solution is to do this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">groupby</span>

<span class="n">S</span><span class="o">=</span><span class="nf">input</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">g</span><span class="p">)),</span><span class="nf">int</span><span class="p">(</span><span class="n">k</span><span class="p">))</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">g</span> <span class="ow">in</span> <span class="nf">groupby</span><span class="p">(</span><span class="n">S</span><span class="p">)])</span>
</code></pre></div></div> <ul> <li>combinations():</li> </ul> <p>In “Iterable and iterators” challenge, we have to find the probability that at least one of the K indices selected contains the letter:’a’. To do this, we need to use itertools.combinations() that unlike itertools.permutations(), does not take into account the order of tuple elements. One possible solution for this challenge is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>

<span class="n">N</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
<span class="n">elements</span><span class="o">=</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">()</span>
<span class="n">K</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>

<span class="n">indices</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="k">if</span> <span class="n">elements</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">]</span>
<span class="n">comb</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="nf">combinations</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="n">K</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span> <span class="nf">len</span><span class="p">([</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">comb</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nf">set</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">comb</span><span class="p">))</span>
</code></pre></div></div> <p> <br/> </p> <h1 id="maths">Maths</h1> <ul> <li>complex() and cmath:</li> </ul> <p>cmath is a module providing mathematical functions for complex numbers. It can be used to get module of phase of complex numbers using <em>abs()</em> and <em>cmath.phase()</em> in order to convert numbers from and to polard coordinates.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">cmath</span> <span class="kn">import</span> <span class="n">phase</span>

<span class="n">z</span><span class="o">=</span><span class="nf">complex</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="nf">phase</span><span class="p">(</span><span class="n">z</span><span class="p">))</span> 
</code></pre></div></div> <p> <br/> </p> <h1 id="string">String</h1> <ul> <li>str.swapcase():</li> </ul> <p>Simple method returning a copy of the string with uppercase characters converted to lowercase and vice versa. It behaves as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">swap_case</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">new_s</span><span class="o">=</span><span class="sh">''</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">s</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_</span><span class="o">==</span><span class="n">_</span><span class="p">.</span><span class="nf">upper</span><span class="p">():</span>
            <span class="n">new_s</span><span class="o">+=</span><span class="n">_</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_s</span><span class="o">+=</span><span class="n">_</span><span class="p">.</span><span class="nf">upper</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">new_s</span>
</code></pre></div></div> <ul> <li>str.isalnum(), str.isalpha(), str.isdigit(), str.islower(), str.isupper():</li> </ul> <p>Check whether each character of the string is alphanumeric (a-z, A-Z, 0-9), alphabetical (a-z, A-Z), is a digit (0-9), is lowercase, is uppercase.</p> <ul> <li>str.capitalize():</li> </ul> <p>Transforms the first character of the string into an upper case character. It is equivalent to:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">capitalize</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">splitted</span><span class="o">=</span><span class="n">s</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="p">[:</span><span class="mi">1</span><span class="p">].</span><span class="nf">upper</span><span class="p">()</span> <span class="o">+</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">splitted</span><span class="p">))</span> <span class="p">)</span>
</code></pre></div></div> <p> <br/> </p> <h1 id="regular-expressions">Regular expressions</h1> <p>According to the Python library, the <em>re</em> module provides regular expression matching operations similar to those found in Perl. Regular expressions can help in many situations when we have to verify if something is contained in a string or more generally, when a string follows a certain pattern.</p> <ul> <li>re.findall():</li> </ul> <p>The expression re.findall() returns all the non-overlapping matches of patterns in a string as a list of strings.</p> <p>In a challenge, we had to find every pattern containing two or more vowels lying between two consonnants. To look behind a pattern, we use “?&lt;=”, to look ahead we use “?=”. Moreover, since we have to find two or more vowels, we need to specify “{2,}”. A possible solution might be the following one:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="n">s</span><span class="o">=</span><span class="nf">input</span><span class="p">()</span>
<span class="n">res</span><span class="o">=</span><span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">(?&lt;=[qwrtypsdfghjklzxcvbnmQWRTYPSDFGHJKLZXCVBNM])([aeiouAEIUO]{2,})(?=[qwrtypsdfghjklzxcvbnmQWRTYPSDFGHJKLZXCVBNM])</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <ul> <li> <p>re.finditer(): The expression re.finditer() returns an iterator yielding MatchObject instances over all non-overlapping matches for the re pattern in the string.</p> <ul> <li>re.start() and re.end(): MatchObject comes with additional usefull features such as <em>.start()</em> and <em>.end()</em> These expressions return the indices of the start and end of the substring matched by the group. They are usefull when we have to find the indices of the start and end of a substring in a string.</li> </ul> </li> <li> <p>re.search():</p> </li> </ul> <p>Scan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object.</p> <p>It can be used in the “Detect floating point number” challenge where we need to find if a the next string represents a floating point number.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="n">N</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
<span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="c1"># ^: beginning of the string
</span>    <span class="c1"># $: end of the string
</span>    <span class="c1"># []: character class
</span>    <span class="c1"># \d: matches digit from 0 to 9
</span>    <span class="c1"># *: any quantity
</span>    <span class="nf">print</span><span class="p">(</span><span class="bp">True</span> <span class="k">if</span> <span class="n">re</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">^[-+]?\d*[.]\d*$</span><span class="sh">'</span><span class="p">,</span> <span class="nf">input</span><span class="p">())</span> <span class="k">else</span> <span class="bp">False</span> <span class="p">)</span>
</code></pre></div></div> <p>It can be used in the “Validate list of email address with filter” challenge where we need to determine if an email is valid or not:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="c1"># \w: Matches Unicode word characters.
</span>    <span class="c1"># +: 1 or more repetitions of the preceding RE.
</span>    <span class="c1"># {1,3}: at least 1, up to 3.
</span>    <span class="c1"># $: end of the string.
</span>    <span class="k">return</span> <span class="bp">True</span> <span class="k">if</span> <span class="n">re</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="sh">"</span><span class="s">^[\w-]+@[a-zA-Z0-9]+\.[a-zA-Z]{1,3}$</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">else</span> <span class="bp">False</span>
</code></pre></div></div> <p>It can be used on the “Validating credit card” challenge where we need to guess if the credit card is valid or not.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="n">n</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">cc</span><span class="o">=</span><span class="nf">input</span><span class="p">()</span>
    <span class="c1"># ^[456]: begins with 4,5 or 6.
</span>    <span class="c1"># \d{3}: contains three digits
</span>    <span class="c1"># (-?\d{4}){3}: might contains - and have four digits. The pattern is repeated 3 times.
</span>    <span class="c1"># \1: refers to the first capturing group
</span>    <span class="c1"># (\d)(-?\1){3}: defines a digit group, then a group containing either - or a digit.
</span>    <span class="c1"># The pattern is repeated 3 times.
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="nf">fullmatch</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">^[456]\d{3}(-?\d{4}){3}$</span><span class="sh">"</span><span class="p">,</span> <span class="n">cc</span><span class="p">)</span> <span class="ow">and</span> \
         <span class="ow">not</span> <span class="n">re</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">(\d)(-?\1){3}</span><span class="sh">"</span><span class="p">,</span> <span class="n">cc</span><span class="p">)):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Valid</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>re.match():</li> </ul> <p>According to <a href="https://www.geeksforgeeks.org/python-re-search-vs-re-match/"> geeksforgeeks </a>, re.match() searches only from the beginning of the string and return match object if found. But if a match of substring is found somewhere in the middle of the string, it returns none. While re.search() searches for the whole string even if the string contains multi-lines and tries to find a match of the substring in all the lines of string.</p> <p>It can be used on the “Validate roman problem” challenge where we need to guess if the roman number is valid or not.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="c1"># (M,{,3}): thousands from 1000 to 3000.
# (C[DM]|D?C{0,3}): either 900 CM, 400 CD or 100-300 C-CCC and 500-800 D-DCCC, etc.
</span><span class="n">regex_pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">"</span><span class="s">(M{,3})(C[DM]|D?C{0,3})(X[LC]|L?X{0,3})(I[VX]|V?I{0,3})$</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">bool</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="n">regex_pattern</span><span class="p">,</span> <span class="nf">input</span><span class="p">()))))</span>
</code></pre></div></div> <ul> <li>re.split():</li> </ul> <p>Split the string by the matches of a regular expression. Inspired from the challenge ‘re.split()’, we can have a usefull folder separator:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">S</span><span class="o">=</span><span class="sh">'</span><span class="s">user/works/python/awesomefile.py</span><span class="sh">'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">[/]</span><span class="sh">'</span><span class="p">,</span><span class="n">S</span><span class="p">)</span>
<span class="p">[</span><span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">works</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">python</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">awesomefile.py</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <ul> <li>re.sub():</li> </ul> <p>Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl. The challenge ‘Regex Substitution’ asks to replace “&amp;&amp;” and “||” by “and” and “or”. To avoid nested re.sub(), one possible solution is to use match.group() and a replacement function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>

<span class="c1"># (?&lt;= ): should begin by a blank space.
# (&amp;&amp;|\|\|): should contain either double &amp; or double |
# (?&lt;= ): should end by a blank space.
</span><span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">(?&lt;= )(&amp;&amp;|\|\|)(?= )</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ls</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="sh">'</span><span class="s">and</span><span class="sh">'</span> <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="sh">"</span><span class="s">&amp;&amp;</span><span class="sh">"</span> <span class="k">else</span> <span class="sh">'</span><span class="s">or</span><span class="sh">'</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">input</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="n">ls</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

</code></pre></div></div> <p> <br/> </p> <h1 id="datetime">Datetime</h1> <ul> <li>datetime.strptime():</li> </ul> <p>It creates a datetime object from a string representing a date and time and a corresponding format string. The format must follows conventions: %a represents abbreviated weekdays, %d represents day of the month as a zero-padded decimal number, etc.</p> <p>Using this function, the <em>Time delta challenge</em> can be solved with the following code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">time_delta</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">):</span>
    <span class="c1"># represented pattern: Day dd Mon yyyy hh:mm:ss +xxxx
</span>    <span class="n">fmt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">%a %d %b %Y %H:%M:%S %z</span><span class="sh">"</span>
    <span class="n">t1_dt</span> <span class="o">=</span> <span class="n">dt</span><span class="p">.</span><span class="nf">strptime</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">fmt</span><span class="p">)</span>
    <span class="n">t2_dt</span> <span class="o">=</span> <span class="n">dt</span><span class="p">.</span><span class="nf">strptime</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="n">fmt</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">str</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="nf">int</span><span class="p">((</span><span class="n">t1_dt</span> <span class="o">-</span> <span class="n">t2_dt</span><span class="p">).</span><span class="nf">total_seconds</span><span class="p">())))</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="challenge"/><category term="python"/><summary type="html"><![CDATA[What did I learn solving all Hackerrank's Python questions ?]]></summary></entry></feed>