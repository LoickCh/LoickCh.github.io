<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Torch module explained (torch.autograd) | Loick Chambon</title> <meta name="author" content="Loick Chambon"/> <meta name="description" content="Exploring and explaining torch functionalities."/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://loickch.github.io/blog/2022/Torch-autograd/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://loickch.github.io/"><span class="font-weight-bold">Loick</span> Chambon</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Torch module explained (torch.autograd)</h1> <p class="post-meta">October 24, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a>     ·   <a href="/blog/category/library"> <i class="fas fa-tag fa-sm"></i> library</a>   </p> </header> <article class="post-content"> <p>Today we will dive into torch.autograd module, one of them most important under the hood PyTorch’s module. It implements automatic differentiation of arbitrary scalar valued functions.</p> <p> <br> </p> <h1 id="table-of-contents">Table of contents:</h1> <ul> <li>How to calculate gradients ?</li> <li>How to locally disable gradients ?</li> <li>How to measure performance ?</li> </ul> <p> <br> </p> <h1 id="how-to-calculate-gradients-">How to calculate gradients ?</h1> <p>Torch.autograd performs automatic differentiation using two different modes: reverse mode and forward mode.</p> <p>The popularity of reverse mode in deep learning comes from the fact that in general the input dimensionality is higher than the output dimensionality. But theorically, for any \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\), the reverse mode should be used if and only if \(n&gt;&gt;m\), otherwise if \(n&lt;&lt;m\), forward mode is more performant.</p> <p>The reason behind is the order we multiply Jacobian matrices in the chain rule. Indeed, considering real function f and g such that \(f: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_1}\) and \(g: \mathbb{R}^{n_1} \rightarrow \mathbb{R}^{n_2}\) where \((n_0,n_1,n_2) \in \mathbb{N}^3\). Then, if we consider the derivative of \(y= g \circ f\) on \(x \in \mathbb{R}^{n_0}\), we have:</p> \[\frac{\partial y}{\partial x}(x) = \frac{\partial y}{\partial g}(g(x)).\frac{\partial g}{\partial f}(f(x)).\frac{\partial f}{\partial x}(x)\] <p>In multi-dimensional space, the analogy of derivatives is the Jacobian matrix defined for any \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\) by:</p> \[J_f= \begin{bmatrix} \frac{\partial f_1}{x_1} ... \frac{\partial f_1}{x_n} \\ ...\\ \frac{\partial f_m}{x_1} ... \frac{\partial f_m}{x_n} \end{bmatrix}\] <p>And the chain rule can be re-written, for any \(f: \mathbb{R}^{n_1} \rightarrow \mathbb{R}^{n_2}\) and \(g: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_1}\) two real functions, as:</p> \[J_{f\circ g} = \underset{n_2 \times n_1}{J_{f}(g)}.\underset{n_1 \times n_0}{J_g}\] <p>This implies we can rewrite the Jacobian matrix of a composition as a product of two matrices. If we have a composition of several functions, we have to calculate a matrix product.</p> <p>The optimised way of calculating the matrix product is not always straightforward, it depends on the dimensions of the matrix. If we multiply matrices from right to left, we perform forward AD, otherwise if we multiply matrices from left to right, we perform backward AD.</p> <h3 id="example">Example</h3> <p>Let us calculate the gradient of a two layer MLP written as functions defined by \(h=f_3 \circ f_2 \circ f_1\) where \(f_1: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_1}\), \(f_2: \mathbb{R}^{n_1} \rightarrow \mathbb{R}^{n_2}\) and \(f_3: \mathbb{R}^{n_2} \rightarrow \mathbb{R}^{n_3}\) such that:</p> \[\forall i \in \{1,2,3\}, \forall x\in \mathbb{R}^{n_{i-1}}, f_i(x)=x.W_i^T + b_i\] <p>Where:</p> \[\forall i \in \{1,2,3\}, W_i \in \mathbb{R}^{n_i \times n_{i-1}}, b_i\in \mathbb{R}^{n_i}\] <p>We have:</p> \[\forall i \in [[1,...n_0]], \frac{\partial f_1}{\partial x_i} = \frac{\partial x.W_{1}^{T}}{\partial x_i} = L_i^{*1}\] <p>Hence:</p> \[J_{f_1}(a) = (L_1^{*1}, ..., L_{n_0}^{*1}) = (C_1^{1}, ..., C_{n_0}^{1}) = W_1 \in \mathbb{R}^{n_1 \times n_0}\] <p>Using the chain rule,</p> \[J_h(a) =J_{f_3 \circ f_2 \circ f_1}(a)= J_{f_3}(f_2 \circ f_1(a)).J_{f_2}(f_1(a)).J_{f_1}(a) = W_3. W_2 . W_1\] <p>So the Jacobian is the product of the three weight matrices. We can calculate it using from left to right product (reverse mode) or from right to left product (forward mode).</p> <h2 id="reverse-mode">Reverse mode</h2> <p>In PyTorch, reverse mode automatic differentation is the by default mode. To calculate gradients, <em>torch.autograd</em> contains two main functions:</p> <ul> <li> <em>backward()</em>: convenient when working with a torch.nn model since it does not require the user to specify which Tensors we want the gradient for. It adds a <em>.grad</em> field to the leaf tensors.</li> <li> <em>grad()</em>: does not add a grad field and requires to specify the Tensors we want the gradients for.</li> </ul> <h3 id="theory">Theory</h3> <p>To perform reverse AD, PyTorch records a computation graph containing all the operations to go from inputs to outputs. It gives us a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. The graph is created in forward pass, while gradients are calculated in backward pass using the computation graph.</p> <p>One important thing to note is that intermediary results are saved during the forward pass in order to execute the backward pass. Those intermediary tensors can increase GPU memory consumption on training.</p> <p>Let us see an example with a simple linear layer. We use torchviz to visualize execution graphs and traces. This code must be launched in a notebook after having installed graphviz and torchviz.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>graphviz   
pip <span class="nb">install </span>torchviz
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Imports:
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torchviz</span> <span class="kn">import</span> <span class="n">make_dot</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="c1"># Params
</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="mi">2</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="nc">OrderedDict</span><span class="p">([</span> <span class="p">(</span><span class="sh">'</span><span class="s">Lin1</span><span class="sh">'</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">out_f</span><span class="p">))</span> <span class="p">])</span> <span class="p">)</span>

<span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">make_dot</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">params</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">()),</span> <span class="n">show_attrs</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_saved</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="row justify-content-md-center"> <div class="col-sm-4"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/2022-10-24/output-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/2022-10-24/output-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/2022-10-24/output-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/Blog/2022-10-24/output.png"> </picture> </figure> </div> <div class="col-sm"> <font color="blue"> Blue boxes:</font> represent tensors requiring a gradient computation. We can see the linear weight matrix of shape (2,16) and the linear bias matrix of shape (2). <br> <br> <font color="orange"> Orange boxes:</font> represent intermediary tensors. Those are tensors saved during the forward pass that enables to calculate gradients during the backward. <br> <br> <font color="grey"> Grey boxes:</font> minimal transformations and traces. Let us explain some boxes: <ul> <li> <i> AccumulateGrad </i>: indicates that gradients need to be calculated. Since weight and bias matrices are encoded as nn.Parameter, they have requires_grad set to True and appear in blue while the input has requires_grad set to False and does not appear;</li> <li> <i> TBackward0 </i>: refers to the transposed operation (W<sup>T</sup>);</li> <li> <i>MmBackward0</i>: refers to the matrix multiplication (x.W<sup>T</sup>);</li> <li> <i>AddBackward0</i>: refers to the addition (x.W<sup>T</sup> + b);</li> </ul> </div> </div> <p>Intermediary tensors imply additional memory cost during training. It is one possible reason explaining why a model fit in memory during evaluation and not during training. These intermediary results are packed during the forward pass (saved) and unpack during the backward pass (when access to the tensor is required). If we want to control the packing/ unpacking behaviour, we can use the <em>torch.autograd.graph.saved_tensors_hooks()</em> context manager.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Call every time a tensor is saved: forward pass
</span><span class="k">def</span> <span class="nf">pack_hook</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Packing</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Call every time a tensor is accessed: backward pass
</span><span class="k">def</span> <span class="nf">unpack_hook</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Unpacking</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="nf">saved_tensors_hooks</span><span class="p">(</span><span class="n">pack_hook</span><span class="p">,</span> <span class="n">unpack_hook</span><span class="p">):</span>
    <span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># Call pack_hook and defined unpack_hook
</span><span class="n">y</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># Call unpack_hook
</span></code></pre></div></div> <p>If this feature is useful for debugging and adds modularity but what is more interesting is to control wether the intermediary tensors are saved on the cpu or on the GPU because very often, the tensors involved in the computation graph live on GPU. Keeping a reference to those tensors in the graph is what causes most models to run out of GPU memory during training while they would have done fine during evaluation. If it is possible to do it using <em>pack_hook</em>, <em>unpack_hook</em>, PyTorch provides another context manager <em>torch.autograd.graph.save_on_cpu()</em>. During the forward pass, tensors will be stored on CPU, while during backward pass, they will be put to the GPU.</p> <h3 id="example-1">Example:</h3> <p>We continue the theorical example of the composition of three nested linear layers to verify that the Jacobian of the composition is the product of the weight matrices. To do this, we can either use the <em>torch.autograd.grad()</em> or <em>torch.autograd.backward().</em></p> <p>Let us begin with “backward”. According to the documentation, it computes the sum of gradients with respect to graph leaves. We note that it has an argument called <em>grad_tensors</em> specifying the gradient of the output. It multiplies the result on the left by <em>grad_tensors</em>. If we note it <em>G</em> and \(f\) the function we are calculating the jacobian, then the result will of <em>torch.autograd.backward</em> is: \(G . J_f\)</p> <p>For instance, if \(G\) is equals to a tensor full of one, we have:</p> \[(1, ... , 1) . J_f = \Big(\sum_{i=1}^{m} \frac{\partial f_i}{x_j} \Big)_{j \in \{1,...,n\}}\] <p>In practice,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Imports:
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Main:
</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">2</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="o">=</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">))</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="p">)</span>

<span class="c1"># Forward
</span><span class="n">y</span><span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">grad_output</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">out_f</span><span class="p">)</span>

<span class="c1"># Backward: sum of gradients
</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">grad_tensors</span><span class="o">=</span><span class="n">grad_output</span><span class="p">)</span>
<span class="n">jacob_sum</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span>

<span class="c1"># Sanity check
</span><span class="n">prod</span><span class="o">=</span><span class="n">lin_out</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_inter</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_in</span><span class="p">.</span><span class="n">weight</span> <span class="c1"># W_3.W_2.W_1
</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jacob_sum</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">prod</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># Equality !
</span></code></pre></div></div> <p>The <em>torch.autograd.grad()</em> counterpart is really close to the previous implementation and rather straightforward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Change this
# torch.autograd.backward(y, grad_tensors=grad_output)
# jacob_sum=x.grad
# By
</span><span class="n">jacob_sum</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_output</span><span class="p">)</span>
</code></pre></div></div> <h2 id="forward-mode">Forward mode</h2> <p>PyTorch 1.12 has a beta API implementing forward mode AD. It contains three main functions:</p> <ul> <li>foward_ad.dual_level(): a context manager ensuring that all dual tensors created inside will have their tangents destroyed upon exit. Convenient to avoid confusing tangents from different computations.</li> <li>foward_ad.make_dual(): create a dual tensor associating a primal tensor with a dual tensor. The name comes from the dual number theory.</li> <li>foward_ad.unpack_dual(): unpacks a dual tensor giving its Tensor value and its forward AD gradient.</li> </ul> <h3 id="theory-1">Theory</h3> <p>We have already explained that the theorical difference between reverse and forward mode is the way we compute matrix product. In practice, we can not use the same pipeline, i.e create a computational graph and backpropagate. To implement forward AD, we need a bit of dual number theory.</p> <p>Let us write any dual number \(x=a+b. \epsilon\) where \(\epsilon^2=0\). When we apply, for any real function, the Taylor series on a dual number, we have: \(f(a+b\epsilon)=\sum_{n=0}^{\infty} \frac{f^{(n)}(a) b^n \epsilon^n }{n!} = f(a)+b.f^{'}(a).\epsilon\) since \(\epsilon^2=0\).</p> <p>Hence, for any real function g, we have:</p> \[f(g(x)) = f(g(a) + b.g^{'}(a)\epsilon)= f(g(a)) + b.g^{'}(a).f^{'}(g(a)).\epsilon\] <p>Moreover, we have for any real function f and g:</p> \[f(x).g(x)=(f(a)+b.f^{'}(a).\epsilon).(g(a)+b.g^{'}(a).\epsilon) =f(a).g(a) + (b.f^{'}(a).g(a)+f(a).b.g^{'}(a)).\epsilon\] <p>The previous results are sufficient to calculate derivatives. For instance, if \(f: \mathbb{R} \rightarrow \mathbb{R}\) such that \(\forall x \in \mathbb{R}, f(x)=x^2.sin(x)\). If we want to calculate the derivative on 3, we do the following calculus:</p> \[x=3+1.\epsilon, x^2=9+6.\epsilon, sin(x)=sin(3)+cos(3).\epsilon\] \[f(x)=x^2.sin(x)=(9+6.\epsilon).(sin(3)+cos(3)\epsilon)= 9.sin(3)+ (9.cos(3)+6.sin(3)). \epsilon\] <p>Hence,</p> \[f(3)=9.sin(3) \text{ and } f^{'}(3)=(9.cos(3)+6.sin(3))\] <h3 id="example-2">Example</h3> <p>We use the same example as before and keep the notations of the official introductory notebook: primal (input, previously “x”) and tangent (input derivatives). Using the same notations as before, tangent <em>T</em> applies the following transformation on the Jacobian: \(J_f . T\)</p> <p>Thus, if \(T\) is equals to a tensor full of one, we have:</p> \[J_f . \begin{pmatrix} 1 \\ ... \\ 1 \end{pmatrix} = \Big(\sum_{j=1}^{n} \frac{\partial f_j}{x_i} \Big)_{i \in \{1,...,m\}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Params
</span><span class="n">in_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">2</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tangent</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">in_f</span><span class="p">)</span>
<span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="o">=</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">inter_f</span><span class="p">),</span> 
                            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">inter_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">))</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">lin_in</span><span class="p">,</span> <span class="n">lin_inter</span><span class="p">,</span> <span class="n">lin_out</span><span class="p">)</span>

<span class="c1"># Forward
</span><span class="k">with</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">dual_level</span><span class="p">():</span>
    <span class="n">dual_input</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">make_dual</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">dual_input</span><span class="p">)</span>
    <span class="n">jvp</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">unpack_dual</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">tangent</span>

<span class="c1"># Sanity check
</span><span class="n">prod</span><span class="o">=</span><span class="n">lin_out</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_inter</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">lin_in</span><span class="p">.</span><span class="n">weight</span> <span class="c1"># W_3.W
</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jvp</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">prod</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># Equality !
</span></code></pre></div></div> <p>The result is the same as before !</p> <p> <br> </p> <h2 id="reverse-ad-vs-forward-ad">Reverse AD vs forward AD</h2> <p>In the introduction, I presented the interest of the AD forward method when the dimensionality of the output space is higher than that of the input space. It is time to verify the statement in coding.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="mi">16</span>

<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">)</span>

<span class="c1"># Forward AD
</span><span class="n">primal</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
<span class="n">tangents</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">jacob_fwd</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="n">tangents</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">dual_level</span><span class="p">():</span>
        <span class="n">dual_input</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">make_dual</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">dual_input</span><span class="p">)</span>
        <span class="n">jvp</span> <span class="o">=</span> <span class="n">fwAD</span><span class="p">.</span><span class="nf">unpack_dual</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">tangent</span>
    <span class="n">jacob_fwd</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">jvp</span><span class="p">)</span>
<span class="n">jacob_fwd</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">jacob_fwd</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Forward AD: </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="o">&gt;</span><span class="mf">21.5</span><span class="n">f</span><span class="si">}</span><span class="s">s shape </span><span class="si">{</span><span class="n">jvp</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Reverse AD
</span><span class="n">inp</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">gradients</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="n">jacob_rev</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">grad_tensors</span><span class="o">=</span><span class="n">grad</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">jacob_rev</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">inp</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">jacob_rev</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">jacob_rev</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Backward AD: </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="o">&gt;</span><span class="mf">20.5</span><span class="n">f</span><span class="si">}</span><span class="s">s shape </span><span class="si">{</span><span class="n">inp</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jacob_rev</span><span class="p">,</span> <span class="n">jacob_fwd</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div> <p>One could use more advanced functions to criticise the use of ‘for’ loops. However, this code gives us a first impression of the time difference and is not intended to be as optimised as possible. Nonetheless, results are striking. The difference in computation time between the two approaches is of several magnitudes depending on whether the final dimension is greater than the initial one.</p> <p>This result is very important because if I had said that the popularity of reverse AD in deep learning was due to the fact that the output dimension is usually smaller than the input dimension, there are problems and architectures (encoder) that do not verify this result.</p> <p>One can hope that one day the optimisation will intelligently choose which mode to use according to the intermediate spaces of the architecture.</p> <h2 id="optimized-comparison">Optimized comparison</h2> <p>The last code snippet, give us a hint about the best mode depending on the situation. However, it was a bit tedious. That is why, PyTorch has implemented optimized functions in <em>torch.autograd.functional</em> to calculate Jacobians (1st order) and Hessians (2nd order) in a beta API. PyTorch is trying to catch up with JAX on the optimized calculation of high-order gradients.</p> <h3 id="example-3">Example:</h3> <p>For instance, instead of using a for loop to calculate jacobian, we can use a one-line function. The results are faster but the conclusion is the same: the time difference depends on the distance between the dimensions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="mi">1024</span>

<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">)</span>

<span class="c1"># Forward 
</span><span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">jacob_fwd</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">vectorize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">forward-mode</span><span class="sh">'</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Time: {:10.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>

<span class="c1"># Reverse
</span><span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="n">jacob_rev</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Time: {:10.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">jacob_rev</span><span class="p">,</span> <span class="n">jacob_fwd</span><span class="p">)</span>
</code></pre></div></div> <p> <br> </p> <h1 id="how-to-locally-disable-gradients-">How to locally disable gradients ?</h1> <p>There are two common situations where we want to disable gradients: during inference or using custom code. Different context manager help us to globally disable or enable gradient calculation and storage. There are three different context:</p> <ul> <li>grad mode: it is the by default mode and is the only mode in which <em>requires_grad</em> takes effect. It is used when training models.</li> <li>no-grad mode (<em>torch.set_grad_enabled(False)</em> or <em>torch.no_grad()</em>): convenient to temporary disable the tracking of any operations requires to latter calculate gradients without having to set <em>requires_grad</em> to False and then back to True.</li> <li>inference mode (<em>torch.inference_mode()</em>): optimized version of no-grad mode disabling view tracking and version counter bumps. It is used in during data processing and evaluation.</li> </ul> <p><strong>Note:</strong></p> <ul> <li>model.eval() is not a context manager. It only change the behaviour of modules acting differently during training and evaluation (nn.Dropout, nn.BatchNorm2d)</li> <li>None of the context manager is available for forward AD.</li> </ul> <p> <br> </p> <h1 id="how-to-measure-performance-">How to measure performance ?</h1> <h2 id="profiling">Profiling</h2> <p>Profiling code is really important to optimize code, find bottlenecks, check GPU memory, check CPU memory, etc. PyTorch has implemented a profiler recording GPU and CPU events with a simple context manager <em>torch.autograd.profiler.profile()</em>. Originally it was part of <em>torch.autograd</em> module since it deals with code optimisation. But, with PyTorch 1.8.1 release, the module is considered legacy and is deprecated in favour of <em>torch.profiler</em> module.</p> <p>Autograd profiler has many interesting arguments such as:</p> <ul> <li>enabled: allows to enable or not profiling. It is useful so that we do not have to comment on or uncomment the code, whether we want to profile it or not.</li> <li>use_cuda: enables timing of CUDA event.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="mi">1</span>
<span class="n">bool_profile</span><span class="o">=</span><span class="bp">False</span>

<span class="c1"># Setup
</span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="n">out_f</span><span class="p">))</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">profiler</span><span class="p">.</span><span class="nf">profile</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">bool_profile</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">y</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>       

<span class="k">if</span> <span class="n">prof</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Save profiling
</span>    <span class="n">prof</span><span class="p">.</span><span class="nf">export_chrome_trace</span><span class="p">(</span><span class="sh">'</span><span class="s">result_profiling</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Print table order by CPU time 
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">prof</span><span class="p">.</span><span class="nf">key_averages</span><span class="p">().</span><span class="nf">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="sh">"</span><span class="s">self_cpu_time_total</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div> <p>After profiling, we can print table in the terminal directly using a simple print or after having grouped functions using <em>profiler.profile.key_averages</em>. Another intersting tools is to save a trace of the profiling and to load it later in: <em>chrome://tracing</em></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/2022-10-24/tracing-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/2022-10-24/tracing-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/2022-10-24/tracing-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/Blog/2022-10-24/tracing.png" title="chrome://tracing visualisation"> </picture> </figure> </div> </div> <div class="caption"> chrome://tracing visualisation </div> <hr> <h1 id="ressources">Ressources</h1> <p><strong>Concepts:</strong></p> <ul> <li><a href="https://pytorch.org/docs/stable/autograd.html#" target="_blank" rel="noopener noreferrer">AD</a></li> <li><a href="https://towardsdatascience.com/forward-mode-automatic-differentiation-dual-numbers-8f47351064bf" target="_blank" rel="noopener noreferrer">Understand AD</a></li> <li><a href="https://math.stackexchange.com/questions/2195377/reverse-mode-differentiation-vs-forward-mode-differentiation-where-are-the-be" target="_blank" rel="noopener noreferrer">Understand AD (2)</a></li> <li><a href="https://jingnanshi.com/blog/autodiff.html" target="_blank" rel="noopener noreferrer">Mathematical aspects of AD</a></li> </ul> <p><strong>Benchmark:</strong></p> <ul> <li><a href="https://leimao.github.io/article/Automatic-Differentiation/" target="_blank" rel="noopener noreferrer">Comparison between AD modes</a></li> <li><a href="https://leimao.github.io/blog/PyTorch-Automatic-Differentiation/" target="_blank" rel="noopener noreferrer">AD mode time comparison</a></li> </ul> <p><strong>Notebooks:</strong></p> <ul> <li><a href="https://pytorch.org/tutorials/intermediate/forward_ad_usage.html" target="_blank" rel="noopener noreferrer">AD</a></li> <li><a href="https://pytorch.org/docs/stable/notes/autograd.html#" target="_blank" rel="noopener noreferrer">Autograd</a></li> <li><a href="https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html" target="_blank" rel="noopener noreferrer">Autograd Hooks</a></li> </ul> <p><em>News:</em></p> <ul> <li><a href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/" target="_blank" rel="noopener noreferrer">Profiler</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Loick Chambon. Created using the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>